# Plotting the 50-year return level
plot(rl_mle_evd_50)
title(main = "50-Year Return Level")
# Plotting the 100-year return level
plot(rl_mle_evd_100)
title(main = "100-Year Return Level")
# Resetting the plotting layout to default
par(mfrow = c(1, 1))
par(mfrow = c(1, 1))
# Plotting the 50-year return level
plot(rl_mle_evd_50)
title(main = "50-Year Return Level")
# Plotting the 100-year return level
plot(rl_mle_evd_100)
title(main = "100-Year Return Level")
# Resetting the plotting layout to default
par(mfrow = c(1, 1))
par(mfrow = c(1, 1))
plot(rl_mle_evd_50)
title(main = "50-Year Return Level")
# Plotting the 100-year return level
plot(rl_mle_evd_100)
title(main = "100-Year Return Level")
par(mfrow = c(1, 2))
plot(rl_mle_evd_50)
title(main = "50-Year Return Level", line = -1)
plot(rl_mle_evd_100)
title(main = "100-Year Return Level", line = -1)
par(mfrow = c(1, 4))
plot(rl_mle_evd_50)
title(main = "50-Year Return Level", line = -1)
plot(rl_mle_evd_100)
title(main = "100-Year Return Level", line = -1)
par(mfrow = c(2, 2))
plot(rl_mle_evd_50)
title(main = "50-Year Return Level", line = -1)
plot(rl_mle_evd_100)
title(main = "100-Year Return Level", line = -1)
par(mfrow = c(2, 2))
plot(rl_mle_evd_50)
title(main = "50-Year Return Level", line = -1, col.main = "red")
plot(rl_mle_evd_100)
title(main = "100-Year Return Level", line = -1, col.main = "red")
title(main = "50-Year Return Level", line = 4, col.main = "red")
par(mfrow = c(2, 2))
plot(rl_mle_evd_50)
title(main = "50-Year Return Level", line = 4, col.main = "red")
plot(rl_mle_evd_100)
title(main = "100-Year Return Level", line = 4, col.main = "red")
par(mfrow = c(2, 2))
plot(rl_mle_evd_50)
title(main = "50-Year Return Level", line = -1, col.main = "red")
plot(rl_mle_evd_100)
title(main = "100-Year Return Level", line = -1, col.main = "red")
par(mfrow = c(2, 2))
plot(rl_mle_evd_50)
title(main = "50-Year Return Level", line = -1, col.main = "red")
plot(rl_mle_evd_100)
title(main = "100-Year Return Level", line = -1, col.main = "red")
par(mfrow = c(1, 1))
extremalindex(niveau$Wert, threshold_clustering) # 1 / 0.156 = 6.4; we have a very large value, indicating a need to decluster
knitr::opts_chunk$set(echo = TRUE)
night_max <- read_csv(here::here("data/nightmax.csv"))
night_min <- read_csv(here::here("data/nightmin.csv"))
# Remove rows with missing values
night_max <- na.omit(night_max)
night_min <- na.omit(night_min)
# Subset the data for summer months (June to September)
summer_night_max <- night_max[format(night_max$date,"%m") %in% c("06", "07", "08", "09"), ]
```
extremalindex(night_max$night.max, th_1)
# Visualise the data: plot and histogram of the maximum temperatures for the summer months
plot(summer_night_max$date, summer_night_max$night.max, type = "l", xlab = "Date", ylab = "Night Max Temperature", main = "Summer Night Max Temperatures")
hist(summer_night_max$night.max, breaks = 30, xlab = "Night Max", ylab = "Frequency", main = "Histogram of summer night max temperatures")
# Have more information about the distribution
min(summer_night_max$night.max) ; mean(summer_night_max$night.max); max(summer_night_max$night.max)
quantile(summer_night_max$night.max, 0.95) # doesn't seem to be very right-skewed
# Choose the threshold using the mrlplot()
mrlplot(summer_night_max$night.max, main="Mean residual")
threshrange.plot(summer_night_max$night.max, r= c(20,30), nint = 20) # we are going to use 30
th_1 <- 30 # with this threshold we would have 5% of observations: 400 so seems good.
# Visualise the threshold
plot(night_max$night.max, type = 'l')
abline(h=th,col=2) # looks good
# Assess the threshold
pot_mle <- fevd(night_max$night.max, method = "MLE", type="GP", threshold=th_1)
plot(pot_mle)
rl_mle <- return.level(pot_mle, conf = 0.05, return.period= c(2,5,10,20,50,100), do.ci=T) # diagnostic plot looks good and coinfidence interval become wider the longer the return level, which make sense
# Return level plots with MLE
par(mfcol=c(1,1))
plot(pot_mle, type="rl",
main="Return Level Plot for Oberwang w/ MLE",
ylim=c(0,200), pch=16)
loc <- as.numeric(return.level(pot_mle, conf = 0.05, return.period=50))
segments(50, 0, 50, loc, col= 'midnightblue',lty=6)
segments(0.01,loc,50, loc, col='midnightblue', lty=6)
# Assess wether extremes occur in a cluster using extremalindex()
extremalindex(night_max$night.max, th_1) # 0.31 so 1/0.31 = 3.23 WE NEED TO DECLUSTER
extremalindex(night_max$night.max, th_1) # 0.31 so 1/0.31 = 3.23 WE NEED TO DECLUSTER
# return level plot w/ MLE
plot(pot_mle, type="rl",
main="Return Level Plot for Oberwang w/ MLE",
ylim=c(0,200), pch=16)
loc <- as.numeric(return.level(pot_mle, conf = 0.05, return.period=50))
segments(50, 0, 50, loc, col= 'midnightblue',lty=6)
segments(0.01,loc,50, loc, col='midnightblue', lty=6)
confint(profile(pot_decl))
pot_decl <- fpot(as.vector(precipitation_xts), threshold = th, npp = 365.25, cmax = TRUE, ulow = th, mper = 50)
# some changes in the estimates because we have less data, if there is clustering of extremes (expected)
confint(profile(pot_decl))
confint(profile(pot_decl))
niveau_xts <- xts(niveau$Wert, order.by = niveau$Date) # Create an xts object for easier time series manipulation
source(here::here("script/setup.R"))
niveau_xts <- xts(niveau$Wert, order.by = niveau$Date) # Create an xts object for easier time series manipulation
niveau_xts <- xts(niveau$Wert, order.by = niveau$Zeitstempel)
yearly_maxima <- numeric(length(unique(year(niveau$Zeitstempel)))) # Initialize a vector to store maximum values for each year
# decluster and obtain plot, according to years for example
years <- c()
k <- 1
for(i in 1:nrow(precipitation_xts)){
if(is.na(precipitation_xts[i])){
next
}else{
years[k] <- year(precipitation_xts[i])
k <- k+1
}
}
years <- years-1980
for(y in unique(year(niveau$Date))) {
# Extract the data for the year
year_data <- niveau_xts[year(index(niveau_xts)) == y]
# Find the maximum value for the year
yearly_maxima[y - min(year(niveau$Date)) + 1] <- max(year_data, na.rm = TRUE)
}
View(niveau)
yearly_maxima
niveau_xts <- xts(niveau$Wert, order.by = niveau$Zeitstempel) # Create an xts object for easier time series manipulation
yearly_maxima <- numeric(length(unique(year(niveau$Zeitstempel)))) # Initialize a vector to store maximum values for each year
# Loop through each year and find the maximum value
for(y in unique(year(niveau$Date))) {
# Extract the data for the year
year_data <- niveau_xts[year(index(niveau_xts)) == y]
# Find the maximum value for the year
yearly_maxima[y - min(year(niveau$Date)) + 1] <- max(year_data, na.rm = TRUE)
}
yearly_maxima
yearly_maxima
?decluster
decl1
######################################################################################################################
library(xts)
library(extRemes)
library(RCurl)
# get data from eHYD
# adapted from example available at: https://www.gis-blog.com/eva-intro-3/
read.ehyd <- function(ehyd_url) {
# separate the header, open the connection with correct encoding
con <- url(ehyd_url, encoding = "latin1")
header <- readLines(con, n=50)
lines.header <- grep("Werte:", header, fixed = T)
# read data, define time and values
infile <- read.csv2(con, header = F, skip = lines.header,
col.names = c("time", "value"),
colClasses = c("character", "numeric"),
na.strings = "LÃ¼cke",
strip.white = TRUE, as.is = TRUE, fileEncoding = "latin1")
infile$time <- as.POSIXct(infile$time, format = "%d.%m.%Y %H:%M:%S")
# return time series object of class xts
return(xts(infile$value, order.by = infile$time))
}
ehyd_url <- "http://ehyd.gv.at/eHYD/MessstellenExtraData/nlv?id=105700&file=2"
precipitation_xts <- read.ehyd(ehyd_url)
precipitation_xts <- precipitation_xts[-14611,]
# Visualise the data
plot(precipitation_xts)
hist(precipitation_xts, breaks = 30)
# seems to be right-skewed
# MRL plot:
mrlplot(precipitation_xts, main="Mean Residual Life Plot")
View(precipitation_xts)
decl1 <- decluster(as.vector(precipitation_xts), threshold=threshold_clustering, groups=years, na.action=na.omit)
decl1 <- decluster(as.vector(niveau$Wert), threshold =threshold_clustering, groups=years, na.action=na.omit)
decl1 <- decluster(as.vector(niveau$Wert), threshold =threshold_clustering, groups=years, na.action=na.omit)
# 6. Declustering
decl1 <- decluster(as.vector(niveau), threshold =threshold_clustering, groups=years, na.action=na.omit)
?decluster
decl1 <- decluster(as.vector(niveau$Wert), threshold =threshold_clustering, groups=years, na.action=na.omit)
decl1 <- decluster(as.vector(niveau$Wert), threshold = threshold_clustering, groups=years, na.action=na.omit)
library(DataExplorer)
plot_missing(niveau)
years <- c()
k <- 1
for(i in 1:nrow(precipitation_xts)){
if(is.na(precipitation_xts[i])){
next
}else{
years[k] <- year(precipitation_xts[i])
k <- k+1
}
}
years <- years-1980
years <- c()
k <- 1
for(i in 1:nrow(precipitation_xts)){
if(is.na(precipitation_xts[i])){
next
}else{
years[k] <- year(precipitation_xts[i])
k <- k+1
}
}
years <- years-1999
years <- c()
k <- 1
for(i in 1:nrow(niveau)){
if(is.na(niveau[i])){
next
}else{
years[k] <- year(niveau[i])
k <- k+1
}
}
years <- c()
k <- 1
for(i in 1:nrow(niveau$Wert)){
if(is.na(niveau$Wert[i])){
next
}else{
years[k] <- year(niveau$Wert[i])
k <- k+1
}
}
years <- c()
k <- 1
for(i in 1:nrow(niveau)){
if(is.na(niveau$Zeitstempel[i])){
next
}else{
years[k] <- year(niveau[i])
k <- k+1
}
}
years <- c()
k <- 1
for(i in 1:nrow(niveau)){
if(is.na(niveau$Wert[i])){
next
}else{
years[k] <- year(niveau$Zeitstempel[i])
k <- k+1
}
}
years <- years-1999
years
count(years == 1)
years <- c()
k <- 1
for(i in 1:nrow(niveau)){
if(is.na(niveau$Wert[i])){
next
}else{
years[k] <- year(niveau$Zeitstempel[i])
k <- k+1
}
}
years <- years-1999
decl1 <- decluster(as.vector(niveau$Wert), threshold = threshold_clustering, groups=years, na.action=na.omit)
plot(decl1)
plot(decl1)
decl1
plot(decl1)
plot(decl1)
par(mfrow = c(2,2))
plot(pot_mle_evd)
plot(pot_mle)
rl_mle <- return.level(pot_mle, conf = 0.05, return.period= c(2,5,10,20,50,100), do.ci=T)
# alternative using evd
pot_mle_evd <- fpot(as.vector(precipitation_xts), threshold = th, npp = 365.25)
pot_mle_evd2 <- fpot(as.vector(precipitation_xts), threshold = th)
par(mfrow = c(2,2))
plot(pot_mle_evd)
confint(profile(pot_mle_evd))
# return levels with evd, e.g. 50-year
rl_mle_evd <- fpot(as.vector(precipitation_xts), threshold = th, npp = 365.25, mper=50)
plot(rl_mle_evd)
# return level plots
par(mfcol=c(1,1))
# return level plot w/ MLE
plot(pot_mle, type="rl",
plot(pot_mle_evd)
confint(profile(pot_mle_evd))
# return level plot w/ MLE
plot(pot_mle, type="rl",
main="Return Level Plot for Oberwang w/ MLE",
ylim=c(0,200), pch=16)
loc <- as.numeric(return.level(pot_mle, conf = 0.05, return.period=50))
segments(50, 0, 50, loc, col= 'midnightblue',lty=6)
segments(0.01,loc,50, loc, col='midnightblue', lty=6)
rl_mle <- return.level(pot_mle, conf = 0.05, return.period= c(2,5,10,20,50,100), do.ci=T)
rl_mle
rl_mle <- return.level(fitMLE, conf = 0.05, return.period= c(2,5,10,20,50,100), do.ci=T)
rl_mle
# Add decl1 to niveau data frame
niveau$decl1 <- decl1
View(niveau)
# Add years and decl1 to niveau data frame
niveau$years <- years
meplot(niveau$Wert, at = pretty(niveau$Wert, n = 10), labels = format(pretty(niveau$Wert, n = 10), nsmall = 1))
mrlplot(niveau$Wert,main="Mean Residual Life Plot")
quantile(niveau$Wert, 0.95) # 327
threshold_clustering <- 327
night_max <- read_csv(here::here("data/nightmax.csv"))
night_min <- read_csv(here::here("data/nightmin.csv"))
# Remove rows with missing values
night_max <- na.omit(night_max)
night_min <- na.omit(night_min)
# Subset the data for summer months (June to September)
summer_night_max <- night_max[format(night_max$date,"%m") %in% c("06", "07", "08", "09"), ]
# Visualise the data: plot and histogram of the maximum temperatures for the summer months
plot(summer_night_max$date, summer_night_max$night.max, type = "l", xlab = "Date", ylab = "Night Max Temperature", main = "Summer Night Max Temperatures")
hist(summer_night_max$night.max, breaks = 30, xlab = "Night Max", ylab = "Frequency", main = "Histogram of summer night max temperatures")
# Have more information about the distribution
min(summer_night_max$night.max) ; mean(summer_night_max$night.max); max(summer_night_max$night.max)
quantile(summer_night_max$night.max, 0.95) # doesn't seem to be very right-skewed
# Choose the threshold using the mrlplot()
mrlplot(summer_night_max$night.max, main="Mean residual")
threshrange.plot(summer_night_max$night.max, r= c(20,30), nint = 20) # we are going to use 30
th_1 <- 30 # with this threshold we would have 5% of observations: 400 so seems good.
# Visualise the threshold
plot(night_max$night.max, type = 'l')
abline(h=th,col=2) # looks good
# Assess the threshold
pot_mle <- fevd(night_max$night.max, method = "MLE", type="GP", threshold=th_1)
plot(pot_mle)
rl_mle <- return.level(pot_mle, conf = 0.05, return.period= c(2,5,10,20,50,100), do.ci=T) # diagnostic plot looks good and coinfidence interval become wider the longer the return level, which make sense
# Return level plots with MLE
par(mfcol=c(1,1))
plot(pot_mle, type="rl",
main="Return Level Plot for Oberwang w/ MLE",
ylim=c(0,200), pch=16)
loc <- as.numeric(return.level(pot_mle, conf = 0.05, return.period=50))
segments(50, 0, 50, loc, col= 'midnightblue',lty=6)
segments(0.01,loc,50, loc, col='midnightblue', lty=6)
# Assess wether extremes occur in a cluster using extremalindex()
extremalindex(night_max$night.max, th_1) # 0.31 so 1/0.31 = 3.23 WE NEED TO DECLUSTER
?plot
plot(summer_night_max$date, summer_night_max$night.max, type = "b", xlab = "Date", ylab = "Night Max Temperature", main = "Summer Night Max Temperatures")
# 1. Select Threshold: using mean excess plot and quantile
plot(niveau$Wert)
?plot
hist(summer_night_max$night.max, breaks = 30, xlab = "Night Max", ylab = "Frequency", main = "Histogram of summer night max temperatures")
min(summer_night_max$night.max) ; mean(summer_night_max$night.max); max(summer_night_max$night.max)
quantile(summer_night_max$night.max, 0.95) # doesn't seem to be very right-skewed
mrlplot(summer_night_max$night.max, main="Mean residual")
threshrange.plot(summer_night_max$night.max, r= c(20,30), nint = 20) # we are going to use 30
plot(night_max$night.max, type = 'l')
source(here::here("script/setup.R"))
night_max <- read_csv(here::here("data/nightmax.csv"))
night_min <- read_csv(here::here("data/nightmin.csv"))
# Remove rows with missing values
night_max <- na.omit(night_max)
night_min <- na.omit(night_min)
# Subset the data for summer months (June to September)
summer_night_max <- night_max[format(night_max$date,"%m") %in% c("06", "07", "08", "09"), ]
View(summer_night_max)
plot(summer_night_max$date, summer_night_max$night.max, type = "b", xlab = "Date", ylab = "Night Max Temperature", main = "Summer Night Max Temperatures")
hist(summer_night_max$night.max, breaks = 30, xlab = "Night Max", ylab = "Frequency", main = "Histogram of summer night max temperatures")
plot(summer_night_max$night.max, type = 'l')
abline(h=th,col=2) # looks good
# Visualise the threshold
plot(summer_night_max$night.max, type = 'l')
abline(h=th,col=2)
# Visualise the threshold
plot(summer_night_max$night.max, type = 'l')
abline(h=th_1,col=2)
th_1 <- 30 # with this threshold we would have 5% of observations: 400 so seems good.
# Visualise the threshold
plot(summer_night_max$night.max, type = 'l')
abline(h=th_1,col=2)
pot_mle <- fevd(summer_night_max$night.max, method = "MLE", type="GP", threshold=th_1)
plot(pot_mle)
rl_mle <- return.level(pot_mle, conf = 0.05, return.period= c(2,5,10,20,50,100), do.ci=T) # diagnostic plot looks good and confidence interval become wider the longer the return level, which make sense
rl_mle
# Return level plots with MLE
par(mfcol=c(1,1))
plot(pot_mle, type="rl",
main="Return Level Plot for Oberwang w/ MLE",
ylim=c(0,200), pch=16)
loc <- as.numeric(return.level(pot_mle, conf = 0.05, return.period=50))
segments(50, 0, 50, loc, col= 'midnightblue',lty=6)
segments(0.01,loc,50, loc, col='midnightblue', lty=6)
extremalindex(night_max$night.max, th_1) # 0.31 so 1/0.31 = 3.23 WE NEED TO DECLUSTER
# We need to create a vector of size night_max that stores the year.
years <- numeric(nrow(night_max))
k <- 1
for (i in 1:nrow(night_max)) {
if (is.na(night_max$night.max[i])) {
next
} else {
years[k] <- year(as.Date(night_max$date[i]))
k <- k + 1
}
}
years <- years-1999 # we don't want the year but the index of the years
# Use decluster function of extRemes
decl1 <- decluster(night_max$night.max, threshold=th, groups=years, na.action=na.omit) # vector and groups need to have the same size
# We need to create a vector of size night_max that stores the year.
years <- numeric(nrow(night_max))
k <- 1
for (i in 1:nrow(night_max)) {
if (is.na(night_max$night.max[i])) {
next
} else {
years[k] <- year(as.Date(night_max$date[i]))
k <- k + 1
}
}
years <- years-1999 # we don't want the year but the index of the years
# Use decluster function of extRemes
decl1 <- decluster(night_max$night.max, threshold=th_1, groups=years, na.action=na.omit) # vector and groups need to have the same size
decl1 # we have 71 clusters
plot(decl1) # shows in grey the points that are not retained in the selection
years_part3 <- numeric(nrow(night_max))
k <- 1
for (i in 1:nrow(night_max)) {
if (is.na(night_max$night.max[i])) {
next
} else {
years[k] <- year(as.Date(night_max$date[i]))
k <- k + 1
}
}
years_part3 <- years-1999
decl1 <- decluster(night_max$night.max, threshold=th_1, groups=years_part3, na.action=na.omit) # vector and groups need to have the same size
plot(decl1) # shows in grey the points that are not retained in the selection
return_levels_df <- data.frame(return_level = numeric())  # Create an empty data frame with a column "return_level"
for (i in 71:83) {
loc <- gev_time_varying_location_shape$results$par[1] + gev_time_varying_location_shape$results$par[2] * i
scale <- gev_time_varying_location_shape$results$par[3]
shape <- gev_time_varying_location_shape$results$par[4] + gev_time_varying_location_shape$results$par[5] * i
return_level <- qgev(1 - 1/13, location = loc, scale = scale, shape = shape)
return_levels_df <- rbind(return_levels_df, data.frame(return_level = return_level))
}
source(here::here("script/setup.R"))
# Read the Data
venice <- venice90
# Using block maxima approach to have the maximum sea level per year
venice_max <- venice %>%
group_by(year) %>%
summarise(max_sea_level = max(sealevel))
head(venice_max)
plot_ly(venice_max, x = ~year, y = ~max_sea_level, type = 'scatter', mode = 'markers', name = 'Max Value') %>% layout(title = "Maximum Value per year", xaxis = list(title="Year"), yaxis = list(title="Maximum Value"))  %>% add_segments(x = 1940, xend = 2009, y = 140, yend = 140, line = list(color = 'red', width = 2))
# Create linear model
mod1 <- lm(max_sea_level ~ year, data = venice_max)
summary(mod1)
# Predictions of 13 next years using the linear model
mod1_predict <- predict.lm(mod1,newdata=data.frame("year"=c(2010:2022)),se=T, interval = "confidence", level = 0.95)
mod1_predict
# Stored the predictions in a dataframe
venice_max_predict <- data.frame(
PredictedValues = mod1_predict) %>%
mutate(year = c(2010:2022))
head(venice_max_predict)
#Plot the confidence intervals
plotCI(x = venice_max_predict$year,
y = venice_max_predict$PredictedValues.fit.fit,
li = venice_max_predict$PredictedValues.fit.lwr,
ui = venice_max_predict$PredictedValues.fit.upr)
#Create a new dataframe for the extreme values of 2010 - 2022 (table from Wikipedia)
max_real <- data.frame(year = c(2012, 2012, 2013, 2018, 2019, 2019, 2019, 2022), max_sea_level = c(143, 149, 143, 156, 187, 144, 154, 204))
# Create the ggplot object
venice_plot <- ggplot() +
geom_point(data = max_real, aes(x = year, y = max_sea_level, color = "Observed"), alpha = 0.5, show.legend = TRUE, name = "Observed") +
geom_point(data = venice_max_predict, aes(x = year, y = PredictedValues.fit.fit, color = "Predicted"), shape = 1, show.legend = TRUE, name = "Predicted") +
labs(title = "Predicted Yearly Max Values vs Observed Values (>140cm)", x = "Year", y = "Sea Level") +
scale_x_continuous(breaks = unique(c(venice_max_predict$year, max_real$year))) +
scale_color_manual(name = "Data Type", values = c("Observed" = "red", "Predicted" = "black")) +
theme(legend.title = element_blank())
# Convert the ggplot object to a Plotly interactive plot
interactive_venice_plot <- ggplotly(venice_plot)
# Add the confidence interval as a separate trace
interactive_venice_plot <- interactive_venice_plot %>%
add_ribbons(data = venice_max_predict, x = ~year, ymin = ~PredictedValues.fit.lwr, ymax = ~PredictedValues.fit.upr, color = I("blue"), showlegend = TRUE, name = "Confidence Interval")
# Display the interactive plot
interactive_venice_plot
########################################## extRemes ##########################################
# First we unlist our data frame
list_max_sea_levels <- unlist(venice_max$max_sea_level)
# GEV model with fixed location
gev_fix <- fevd(list_max_sea_levels, type = "GEV", time.units= "year")
plot(gev_fix) ; gev_fix$results$par ; return.level(gev_fix) # shape is almost equal to 0.
# Compute confidence interval
ci(gev_fix, type= "parameter") # CI includes 0
# GEV model with varying time-location using both linear and harmonic function
yy <- 1:length(venice_max$year)
gev_time_varying_linear <- fevd(venice_max$max_sea_level, venice_max, type = "GEV", time.units= "year", location.fun = ~ yy)
gev_time_varying_harmonic <- fevd(venice_max$max_sea_level, venice_max, type = "GEV", time.units= "year", location.fun = ~sin(2 * pi * (year - 1940)/70) + cos(2 * pi * (year - 1940)/70))
plot(gev_time_varying_linear) ; plot(gev_time_varying_harmonic) ; gev_time_varying_linear$results$par ; gev_time_varying_harmonic$results$par;  return.level(gev_time_varying_linear) ; return.level(gev_time_varying_harmonic)
# Compare the two models using likelihood ratio test: Ho: no significant difference in model fit H1: there is a significant difference in model fit between the two models.
lrt_result_linear <- lr.test(gev_fix, gev_time_varying_linear) # we can almost reject the null hypothesis at 95% significant level, which make sense as we have indications that the distribution is non-stationary
lrt_result_harmonic <-lr.test(gev_fix, gev_time_varying_harmonic)
attributes(gev_time_varying_linear)
# 589.5372 linear --> this is sufficient as lower AIC
# 594.9147 harmonic --> too complex, we stick with the linear (time varying location)
# Fit a GEV model with time-varying scale
gev_time_varying_scale <-  fevd(venice_max$max_sea_level, venice_max, type = "GEV", time.units = "year", scale.fun = ~ yy) # AIC: 600
gev_time_varying_shape <- fevd(venice_max$max_sea_level, venice_max, type = "GEV", time.units = "year", shape.fun = ~ yy) # AIC:  600.7072
gev_time_varying_scale_shape <- fevd(venice_max$max_sea_level, venice_max, type = "GEV", time.units = "year", shape.fun = ~ yy, scale.fun = ~ yy ) # AIC: 601.9
gev_time_varying_location_scale <- fevd(venice_max$max_sea_level, venice_max, type = "GEV", time.units = "year", scale.fun = ~ yy, location.fun =  ~ yy) # AIC: 590.6
gev_time_varying_location_shape <- fevd(venice_max$max_sea_level, venice_max, type = "GEV", time.units = "year", shape.fun = ~ yy, location.fun =  ~ yy) # AIC: 586
gev_time_varying_scale_shape_location <- fevd(venice_max$max_sea_level, venice_max, type = "GEV", time.units = "year", shape.fun = ~ yy, location.fun =  ~ yy, scale.fun = ~ yy ) # AIC: 586
lrt_result <- lr.test(gev_time_varying_location_shape, gev_time_varying_scale_shape_location) # can't reject the null hypothesis so we decide to choose the gev_time_varying_location_shape
