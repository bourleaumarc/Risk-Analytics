<<<<<<< HEAD
=======
<<<<<<< Updated upstream
), ]
# Subset the winter months (November to February)
#winter_night_min <- night_min[format(night_min$date, "%m") %in% c("11", "12", "01", "02"), ]
# Invert night.min as temperatures are negative
winter_night_min$night.min <- -winter_night_min$night.min
# Visualise the data: plot and histogram
p <- ggplot(winter_night_min, aes(x = date, y = night.min)) +
geom_line() +  # This adds the line type plot
labs(x = "Date", y = "Night Min Temperature", title = "Winter Night Min Temperatures") +
theme_minimal()  # Optional: adds a minimal theme
interactive_plot_winter <- ggplotly(p)
interactive_plot_winter
hist(winter_night_min$night.min, breaks = 30, col = "skyblue", xlab = "Date", ylab = "Night Min Temperature", main = "Winter Night Min Temperatures frequency")
# Have more information about the distribution
min(winter_night_min$night.min); mean(winter_night_min$night.min); max(winter_night_min$night.min)
quantile(winter_night_min$night.min, 0.95)
# Choose the threshold using the mrlplot()
mrlplot(winter_night_min$night.min, main="Mean residual")
threshrange.plot(winter_night_min$night.min, r= c(0,6), nint =20)
th_2 <- 3.3
# Visualise the threshold
plot(winter_night_min$night.min, type = 'l')
abline(h=th_2,col=2) # looks good
# Assess the threshold
pot_mle_2 <- fevd(winter_night_min$night.min, method = "MLE", type="GP", threshold=th_2)
plot(pot_mle_2)
rl_mle <- return.level(pot_mle_2, conf = 0.05, return.period= c(2,5,10,20,50,100), do.ci=T) # diagnostic plot looks good and confidence interval become wider the longer the return level, which make sense
# Return level plots with MLE
par(mfcol=c(1,1))
plot(pot_mle_2, type="rl",
main="Return Level Plot for Oberwang w/ MLE",
ylim=c(0,200), pch=16)
loc <- as.numeric(return.level(pot_mle_2, conf = 0.05, return.period=50))
segments(50, 0, 50, loc, col= 'midnightblue',lty=6)
segments(0.01,loc,50, loc, col='midnightblue', lty=6)
# Assess wether extremes occur in a cluster using extremalindex()
extremalindex(winter_night_min$night.min, th_2) # 0.32 so 1/0.32 = 3.125
########################### Declustering ###########################
# We need to create a vector of size night_min that stores the year.
years_part3_2 <- c()
k <- 1
for (i in 1:nrow(winter_night_min)) {
if (is.na(winter_night_min$night.min[i])) {
next
} else {
years_part3_2[k] <- year(winter_night_min$date[i])
k <- k + 1
}
}
years_part3_2 <- years_part3_2-1999
# Use decluster function of extRemes
decl2 <- decluster(winter_night_min$night.min, threshold=th_2, groups=years_part3_2, na.action=na.omit) # vector and groups need to have the same size
decl2 # we have 71 clusters
plot(decl2) # shows in grey the points that are not retained in the selection
# Fit GPD with clustered and declustered data
# Use fevd of the normal and declustered
gpd_raw_2 <- fevd(winter_night_min$night.min, threshold = th_2, type = "GP")
gpd_declustered_2 <- fevd(decl2, threshold = th_2, type = "GP")
par(mfrow = c(2, 2))
=======
>>>>>>> Stashed changes
plot(gpd_raw_2)
title(main = "GPD fitted to raw data", line = 1, col.main = "red")
plot(gpd_declustered_2)
title(main = "GPD fitted to declustered data", line = 1, col.main = "red")
par(mfrow = c(1, 1))
# Assess the fit: AIC for raw is 38100, AIC for declustered data is 193
# Assuming 'night_min' has a 'date' column in Date format
night_min$year <- as.numeric(format(night_min$date, "%Y"))
night_min$month <- format(night_min$date, "%m")
# Create a new column for the winter season
night_min$winter_season <- ifelse(night_min$month %in% c("01", "02"),
paste(night_min$year - 1, night_min$year, sep="-"),
paste(night_min$year, night_min$year + 1, sep="-"))
# November and December of year t and January and February of year t+1
winter_night_min <- night_min[(night_min$month %in% c("11", "12") &
night_min$year == t) |
(night_min$month %in% c("01", "02") &
night_min$year == t + 1), ]
night_min$year <- as.numeric(format(night_min$date, "%Y"))
night_min$month <- format(night_min$date, "%m")
night_min <- night_min %>%
mutate(
year = as.numeric(format(date, "%Y")),
month = format(date, "%m"),
season_year = ifelse(month %in% c("01", "02"), year - 1, year)
)
# Assuming 'night_min' has a 'date' column in Date format
night_min$year <- as.numeric(format(night_min$date, "%Y"))
night_min$month <- format(night_min$date, "%m")
night_min <- night_min %>%
mutate(
year = as.numeric(format(date, "%Y")),
month = format(date, "%m"),
season_year = ifelse(month %in% c("01", "02"), year - 1, year)
)
# Subset for winter months
winter_night_min <- night_min %>%
filter(
(month %in% c("11", "12") & season_year == year) |
(month %in% c("01", "02") & season_year == year - 1)
)
# Assuming 'night_min' has a 'date' column in Date format
night_min$year <- as.numeric(format(night_min$date, "%Y"))
night_min$month <- format(night_min$date, "%m")
night_min <- night_min %>%
mutate(
year = as.numeric(format(date, "%Y")),
month = format(date, "%m"),
season_year = ifelse(month %in% c("01", "02"), year - 1, year)
)
# Subset for winter months
winter_night_min <- night_min %>%
filter(
(month %in% c("11", "12") & season_year == year) |
(month %in% c("01", "02") & season_year == year - 1)
)
# Subset the winter months (November to February)
#winter_night_min <- night_min[format(night_min$date, "%m") %in% c("11", "12", "01", "02"), ]
# Invert night.min as temperatures are negative
winter_night_min$night.min <- -winter_night_min$night.min
# Visualise the data: plot and histogram
p <- ggplot(winter_night_min, aes(x = date, y = night.min)) +
geom_line() +  # This adds the line type plot
labs(x = "Date", y = "Night Min Temperature", title = "Winter Night Min Temperatures") +
theme_minimal()  # Optional: adds a minimal theme
interactive_plot_winter <- ggplotly(p)
interactive_plot_winter
hist(winter_night_min$night.min, breaks = 30, col = "skyblue", xlab = "Date", ylab = "Night Min Temperature", main = "Winter Night Min Temperatures frequency")
# Have more information about the distribution
min(winter_night_min$night.min); mean(winter_night_min$night.min); max(winter_night_min$night.min)
quantile(winter_night_min$night.min, 0.95)
# Choose the threshold using the mrlplot()
mrlplot(winter_night_min$night.min, main="Mean residual")
threshrange.plot(winter_night_min$night.min, r= c(0,6), nint =20)
th_2 <- 3.3
# Visualise the threshold
plot(winter_night_min$night.min, type = 'l')
>>>>>>> bd978b16ee058f5419bfe0324d45a5811e230fbf
abline(h=th_2,col=2) # looks good
# Assess the threshold
pot_mle_2 <- fevd(winter_night_min$night.min, method = "MLE", type="GP", threshold=th_2)
plot(pot_mle_2)
rl_mle <- return.level(pot_mle_2, conf = 0.05, return.period= c(2,5,10,20,50,100), do.ci=T) # diagnostic plot looks good and confidence interval become wider the longer the return level, which make sense
# Return level plots with MLE
par(mfcol=c(1,1))
plot(pot_mle_2, type="rl",
main="Return Level Plot for Oberwang w/ MLE",
ylim=c(0,200), pch=16)
loc <- as.numeric(return.level(pot_mle_2, conf = 0.05, return.period=50))
segments(50, 0, 50, loc, col= 'midnightblue',lty=6)
segments(0.01,loc,50, loc, col='midnightblue', lty=6)
# Assess wether extremes occur in a cluster using extremalindex()
extremalindex(winter_night_min$night.min, th_2) # 0.32 so 1/0.32 = 3.125
########################### Declustering ###########################
# We need to create a vector of size night_min that stores the year.
years_part3_2 <- c()
k <- 1
for (i in 1:nrow(winter_night_min)) {
if (is.na(winter_night_min$night.min[i])) {
next
} else {
years_part3_2[k] <- year(winter_night_min$date[i])
k <- k + 1
}
}
years_part3_2 <- years_part3_2-1999
# Use decluster function of extRemes
decl2 <- decluster(winter_night_min$night.min, threshold=th_2, groups=years_part3_2, na.action=na.omit) # vector and groups need to have the same size
decl2 # we have 71 clusters
plot(decl2) # shows in grey the points that are not retained in the selection
# Fit GPD with clustered and declustered data
# Use fevd of the normal and declustered
gpd_raw_2 <- fevd(winter_night_min$night.min, threshold = th_2, type = "GP")
gpd_declustered_2 <- fevd(decl2, threshold = th_2, type = "GP")
par(mfrow = c(2, 2))
plot(gpd_raw_2)
title(main = "GPD fitted to raw data", line = 1, col.main = "red")
plot(gpd_declustered_2)
title(main = "GPD fitted to declustered data", line = 1, col.main = "red")
par(mfrow = c(1, 1))
# Assess the fit: AIC for raw is 38100, AIC for declustered data is 193
<<<<<<< HEAD
data(loss) #some insurance data from the copula package
# loss correspond to the loss amount and alae is the allocated loss adjustment expense
# visualise the data
df <- loss[,c(1,2)]
library(copula)
library(psych) #for pairs.panels plots
library(VineCopula)
data(loss) #some insurance data from the copula package
View(loss)
# visualise the data
df <- loss[,c(1,2)] # focus on the first two columns
View(df)
plot(df$loss)
pairs.panels(df, method="kendall")
plot(df) # difficult to assess the structure of dependence on this scale
df.pobs <- pobs(df)
pairs.panels(df.pobs, method="kendall") # hard to read as scales are different. We need to transform to a common scale.
??copula
library(copula)
# Tutorial adapted from https://datascienceplus.com/modelling-dependence-with-copulas/
library(copula)
library(psych) #for pairs.panels plots
library(VineCopula)
data(loss) #some insurance data from the copula package
# loss correspond to the loss amount and alae is the allocated loss adjustment expense
# visualise the data
df <- loss[,c(1,2)] # focus on the first two columns, we have ties (duplicated columns).
plot(df) # difficult to assess the structure of dependence on this scale
# we can't assess the independence / dependnece
plot(df$loss) # kind of exponential but not too much info
pairs.panels(df, method="kendall") #we have a distribution of each of the column. Local regression shows the positive association
# kendall: measure of association.  hard to read as scales are different. We need to transform to a common scale.
# distribution is now uniform in the variables.
# transformation to pseudo-observations
df.pobs <- pobs(df)
pairs.panels(df.pobs, method="kendall") # we have uniform margins. Now i see something helpful. In the right corner there seems to be more positive association. More concentration of points: positive asymptotic dependence.
# copulas give you a way to represent points in a parametric fashion.
#
# We could retransform the data into any new marginals, with the same dependence structure
x1 <- qgamma(df.pobs[,1], shape=2, scale=1)
x2 <- qbeta(df.pobs[,2], shape1=2, shape2=2)
df2 <- cbind(x1, x2)
pairs.panels(df2)
pairs.panels(df2, method="kendall") # not uniform anymore. we have gamma and beta. we keep the same dependence structure.
# Build copula: need to be fitted on uniform margina
cop.t <- tCopula(dim=2) # 2 dimensions because two variables. By default degrees of freedom = 4
# we want to trick the parameters to fit the points
fit.t <- fitCopula(cop.t, df.pobs, method="ml")
fit.t # rho.1 is the pearson correlation, df is the degrees of freedom as fitted
coef(fit.t)
# to know if it's good: the lower the better
aic.t <- 2*2-2*fit.t@loglik # 2*number of parameters - 2*fit.t
??copula
# Build copula 2, specifying an exact form
# first obtain the correlation by the formula in the slides
rho <- sin(cor(df.pobs, method="kendall")*pi/2)[1,2]
# elliptical distribution: very arbitrary
gen.t2 <- mvdc(copula=ellipCopula(family="t", param=rho, df=10), margins=c("gamma","gamma"), # shape of the margins
paramMargins=list(list(shape=4,scale=3), list(shape=3,scale=2)))
gen.t3 <- mvdc(copula=ellipCopula(family="t", param=rho, df=10), margins=c("beta","beta"),
paramMargins=list(list(shape1=2,shape2=1), list(shape1=3,shape2=2)))
gen.t4 <- mvdc(copula=ellipCopula(family="t", param=rho, df=10), margins=c("t","t"),
paramMargins=list(list(df=10), list(df=10)))
fit.t2 <- fitCopula(gen.t2@copula, df.pobs, start=c(param=rho, df=10))
fit.t3 <- fitCopula(gen.t3@copula, df.pobs, start=c(param=rho, df=10))
fit.t4 <- fitCopula(gen.t4@copula, df.pobs, start=c(param=rho, df=10))
aic.t2 <- 2*2-2*attributes(fit.t2)$loglik
aic.t3 <- 2*2-2*attributes(fit.t3)$loglik
aic.t4 <- 2*2-2*attributes(fit.t4)$loglik
# all the aic is the same We just changed the margins but the dependence structure does not change.
# fit a distribution to the margins fitdistr() from MASS this is better.
# advantage of the second approach, even if no change in AIC, is the generator that mvdc provides
rdm.variates <- rMvdc(10000, gen.t2) # generated random observations from a model
plot(rdm.variates)
# Automatic Bivariate copula selection: to see if the copula is good or not.
select.copula <- BiCopSelect(df.pobs[,1], df.pobs[,2], familyset=NA)
select.copula
select.copula$AIC
=======
<<<<<<< Updated upstream
>>>>>>> bd978b16ee058f5419bfe0324d45a5811e230fbf
source(here::here("script/setup.R"))
##### Download daily NAO measurements
NAO.daily <-
fread('ftp://ftp.cdc.noaa.gov/Public/gbates/teleconn/nao.reanalysis.t10trunc.1948-present.txt')
NAO.daily <- as.matrix(NAO.daily)
colnames(NAO.daily) <- c("year","month","day","NAO")
##### Download temperature data: be sure to work in the correct directory
months <- c(12,1,2) #keep only winter observations
temp_max_Zermatt <- read_delim(here::here("data/practical_3/daily_maximum_Zermatt/order_107669_data.txt"),
";", escape_double = FALSE, col_types =
cols(time = col_number()),
trim_ws = TRUE, skip = 1)
colnames(temp_max_Zermatt) <- c("station","time","temp")
temp_max_Zermatt           <- temp_max_Zermatt[-1,]
temp_max_Zermatt[,2]       <- as.Date(apply(temp_max_Zermatt[,2],1,as.character),
"%Y%m%d")
temp_max_Montana <- read_delim(here::here("data/practical_3/daily_maximum_Montana/order_107668_data.txt"), ";", escape_double = FALSE, col_types = cols(time = col_number()), trim_ws = TRUE, skip = 1)
colnames(temp_max_Montana) <- c("station","time","temp")
temp_max_Montana           <- temp_max_Montana[-1,]
temp_max_Montana[,2]       <- as.Date(apply(temp_max_Montana[,2],1,as.character),
"%Y%m%d")
###match the dates of the two time series
temp_max_Montana           <- temp_max_Montana[match(as.matrix(temp_max_Zermatt[,2]),
as.matrix(temp_max_Montana[,2])),]
temp_max_Montana           <- as.matrix(temp_max_Montana)
colnames(temp_max_Montana) <- c("station","time","temp")
temp_max_Zermatt           <- as.matrix(temp_max_Zermatt)
colnames(temp_max_Zermatt) <- c("station","time","temp")
###keep only winter dates
temp_max_Montana <- temp_max_Montana[which(month(as.POSIXlt(temp_max_Montana[,"time"],
format="%Y-%m-%d")) %in% months),]
temp_max_Zermatt <- temp_max_Zermatt[which(month(as.POSIXlt(temp_max_Zermatt[,"time"],
format="%Y-%m-%d")) %in% months),]
Date       <- function( length = 0 ){
newDate = numeric( length )
class(newDate) = "Date"
return(newDate)
}
season_day                   <- yday(as.Date(temp_max_Montana[,2]))
season_day[season_day < 61]  <- season_day[season_day < 61] + 31
season_day[season_day > 334] <- season_day[season_day > 334]- 334
NAO.date <- Date(nrow(NAO.daily))
for(i in 1:nrow(NAO.daily)){
NAO.date[i] <- as.Date(paste(as.character(NAO.daily[i,1]),"-",as.character(NAO.daily[i,2]),
"-",as.character(NAO.daily[i,3]),sep=""),format="%Y-%m-%d")
}
nao  <- NAO.daily[intersect(as.Date(temp_max_Montana[,2]),as.Date(NAO.date)),4]
#Montana
x_Montana          <- data.frame("time"=temp_max_Montana[,2],
"nao"=nao,
"d"=season_day,
"temp"=temp_max_Montana[,3])
as.numeric.factor  <- function(x) {as.numeric(levels(x))[x]}
x_Montana[,"temp"] <- as.numeric(x_Montana[,"temp"])
x_Montana[,"time"] <- as.numeric(year(as.POSIXlt(x_Montana[,"time"], format="%Y-%m-%d")))
x_Montana[,"time"] <- (x_Montana[,"time"]-min(x_Montana[,"time"]))/(max(x_Montana[,"time"])-
min(x_Montana[,"time"]))
#Zermatt
x_Zermatt          <- data.frame("time"=temp_max_Zermatt[,2],
"nao"=nao,
"d"=season_day,
"temp"=temp_max_Zermatt[,3])
as.numeric.factor  <- function(x) {as.numeric(levels(x))[x]}
x_Zermatt[,"temp"] <- as.numeric(x_Zermatt[,"temp"])
x_Zermatt[,"time"] <- as.numeric(year(as.POSIXlt(x_Zermatt[,"time"], format="%Y-%m-%d")))
x_Zermatt[,"time"] <- (x_Zermatt[,"time"]-min(x_Zermatt[,"time"]))/(max(x_Zermatt[,"time"])-
min(x_Zermatt[,"time"]))
Z <- data.frame("Montana"=x_Montana[,"temp"] , "Zermatt"=x_Zermatt[,"temp"], "NAO"=x_Zermatt[,"nao"])
# Scatterplot of Zermatt values against Montana values
plot(Z$Montana, Z$Zermatt, xlab = "Montana Temperature", ylab = "Zermatt Temperature",
main = "Zermatt vs Montana Temperatures")
abline(lm(Z$Zermatt ~ Z$Montana), col = "red")
legend("topright",
legend = "Fitted Regression Line",
col = "red",
lty = 1,
cex = 0.8)
# Create a data frame for Zermatt vs Montana Temperatures
data_Zermatt_Montana <- data.frame(Zermatt = Z$Zermatt, Montana = Z$Montana)
# Chi Plot - Zermatt vs Montana Temperatures
chiplot(data_Zermatt_Montana, main1 = "Chi Plot - Zermatt vs Montana Temperatures")
# Create a data frame for Montana Temperatures vs NAO Index
data_Montana_NAO <- data.frame(Montana = Z$Montana, NAO = Z$NAO)
# Chi Plot - Montana Temperatures vs NAO Index
chiplot(data_Montana_NAO, main1 = "Chi Plot - Montana Temperatures vs NAO Index")
<<<<<<< HEAD
=======
# Assuming your data is stored in a data frame called Z
library(evd)
# Pair 1: Zermatt vs. Montana
pair_1_diff <- Z$Zermatt - Z$Montana
chiplot(cbind(pair_1_diff, pair_1_diff^2))  # Only include differences and squared differences
# Pair 2: Montana vs. NAO
pair_2_diff <- Z$Montana - Z$NAO
chiplot(cbind(pair_2_diff, pair_2_diff^2))  # Only include differences and squared differences
load(here::here("data/practical_2/niveau.Rdata"))
=======
engie <- read.csv("engie.csv")
veolia <- read.csv("veolia.csv")
# Data cleaning and plot
engie <- engie[-c(4456, 4489), ]
veolia <- veolia[-c(4456, 4489), ]
engie$Adj.Close <- as.numeric(engie$Adj.Close)
veolia$Adj.Close <- as.numeric(veolia$Adj.Close)
par(mfrow = c(1,2))
plot(engie$Adj.Close)
plot(veolia$Adj.Close)
# ADF test to asses the stationarity
adf_engie <- adf.test(engie$Adj.Close) #p-value = 0.4466
>>>>>>> bd978b16ee058f5419bfe0324d45a5811e230fbf
library(copula)
library(MASS)
library(tseries)
library(dplyr)
library(plotly)
library(VineCopula)
engie <- read.csv("engie.csv")
veolia <- read.csv("veolia.csv")
# Data cleaning and plot
engie <- engie[-c(4456, 4489), ]
veolia <- veolia[-c(4456, 4489), ]
engie$Adj.Close <- as.numeric(engie$Adj.Close)
veolia$Adj.Close <- as.numeric(veolia$Adj.Close)
par(mfrow = c(1,2))
plot(engie$Adj.Close)
plot(veolia$Adj.Close)
# ADF test to asses the stationarity
adf_engie <- adf.test(engie$Adj.Close) #p-value = 0.4466
adf_veolia <- adf.test(veolia$Adj.Close)#p-value = 0.8228
# Create log return function
calculate_log_returns <- function(stocks) {
log_returns <- -diff(log(stocks))
return(log_returns)
}
# Calculate negative log returns for each index
log_returns_engie <- calculate_log_returns(engie$Adj.Close)
plot(log_returns_engie)
log_returns_veolia <- calculate_log_returns(veolia$Adj.Close)
plot(log_returns_veolia)
plot_engie <- plot_ly(x = engie$Date, y = engie$Adj.Close, type = "scatter", mode = "point", name = "Engie") %>%
layout(title = "Engie")
plot_veolia <- plot_ly(x = veolia$Date, y = veolia$Adj.Close, type = "scatter", mode = "point", name = "Veolia") %>%
layout(title = "Veolia")
combined_plot_2 <- subplot(plot_engie, plot_veolia)
layout(combined_plot_2, title = "Comparison of stock indices")
# Drawing a scatter plot
par(pty="s")
scatter.smooth(x = log_returns_engie, y = log_returns_veolia, xlab = "Engie negative log returns", ylab = "Veolia negative log returns")
# We compute and show the CDF for both negative log returns
cdf_engie <- ecdf(log_returns_engie)
cdf_veolia <- ecdf(log_returns_veolia)
plot(cdf_engie, col = "red", main = "Empirical Cumulative Distributions")
lines(cdf_veolia, col ="blue")
legend("bottomright", legend = c("Engie", "Veolia"), col = c("red", "blue"), lty = 1)
# Number of pseudo-samples
num_samples <- 1000
## Engie
# Generate uniform samples between 0 and 1
u <- runif(num_samples)
# Use the inverse of the ECDF to transform uniform samples to pseudo-samples
pseudo_samples_e <- quantile(cdf_engie, u)
## Veolia
# Generate uniform samples between 0 and 1
u <- runif(num_samples)
# Use the inverse of the ECDF to transform uniform samples to pseudo-samples
pseudo_samples_v <- quantile(cdf_veolia, u)
# Plot pseudo-samples
par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))
plot(sort(pseudo_samples_e), main = "Engie Pseudo-samples", xlab = "Index", ylab = "Log Returns", col = "red", pch = 16)
plot(sort(pseudo_samples_v), main = "Veolia Pseudo-samples", xlab = "Index", ylab = "Log Returns", col = "red", pch = 16)
# Scatter for both pseudo samples
scatter.smooth(x = pseudo_samples_e, y = pseudo_samples_v, xlab = "Engie pseudo", ylab = "Veolia pseudo")
<<<<<<< HEAD
=======
cdf_engie
?ecdf
plot(sort(pseudo_samples_e), main = "Engie Pseudo-samples", xlab = "Index", ylab = "Log Returns", col = "red", pch = 16)
>>>>>>> bd978b16ee058f5419bfe0324d45a5811e230fbf
# Probabilities for cdf_engie and cdf_veoila
u1 <- pobs(log_returns_engie)
u2 <- pobs(log_returns_veolia)
data <- data.frame(engie_prob = u1, veolia_prob = u2)
data_matrix <- as.matrix(data)
# Fit Gaussian copula
gaussian_copula <- normalCopula(dim = 2)
fit_gaussian <- fitCopula(gaussian_copula, data, method = "mpl")
# Fit t copula
t_copula <- tCopula(dim = 2, df = 3)
fit_t <- fitCopula(t_copula, data, method = "mpl")
# Fit Gumbel copula
gumbel_copula <- gumbelCopula(dim = 2)
fit_gumbel <- fitCopula(gumbel_copula, data, method = "mpl")
# Fit Clayton copula
clayton_copula <- claytonCopula(dim = 2, param = 2)
fit_clayton <- fitCopula(clayton_copula, data, method = "mpl")
# Calculate AIC values
aic_values <- AIC(fit_gaussian, fit_t, fit_gumbel, fit_clayton)
aic_values
min(aic_values$AIC) ###the t distribution fits the best our data
<<<<<<< HEAD
BiCopPar2TailDep(family = 2, par = fit_t@copula@parameters[1], par2 = fit_t@copula@parameters[2])
#Weird, need to check!!
View(Z)
View(Z)
View(Z)
plot(Z$NAO)
corr.test(Z$Montana, Z$NAO)
ploz(Z$Montana)
plot(Z$Montana)
# Correlation between Montana Temperatures and NAO Index
cor_Montana_NAO <- cor(Z$Montana, Z$NAO)
cat("Correlation between Montana Temperatures and NAO Index:", cor_Montana_NAO, "\n")
# correlation test
corr.test(Z$Montana, Z$NAO)
cor.test(Z$Montana, Z$NAO)
chiplot(data_Zermatt_Montana, main1 = "Chi Plot - Zermatt vs Montana Temperatures")
# Scatterplot of Zermatt values against Montana values
plot(Z$Montana, Z$Zermatt, xlab = "Montana Temperature", ylab = "Zermatt Temperature",
main = "Zermatt vs Montana Temperatures")
abline(lm(Z$Zermatt ~ Z$Montana), col = "red")
legend("topright",
legend = "Fitted Regression Line",
col = "red",
lty = 1,
cex = 0.8)
# Scatterplot of Montana values against NAO values
plot(Z$Montana, Z$NAO, xlab = "Montana Temperature", ylab = "NAO Index",
main = "Montana Temperature vs NAO Index")
abline(lm(Z$NAO ~ Z$Montana), col = "red") # Add a linear regression line
legend("topright",
legend = "Fitted Regression",
col = "red",
lty = 1,
cex = 0.8)
cor_Zermatt_Montana <- cor(Z$Zermatt, Z$Montana)
cat("Correlation between Zermatt and Montana Temperatures:", cor_Zermatt_Montana, "\n")
cor.test(Z$Zermatt, Z$Montana)
cor_Montana_NAO <- cor(Z$Montana, Z$NAO)
cat("Correlation between Montana Temperatures and NAO Index:", cor_Montana_NAO, "\n")
cor.test(Z$Montana, Z$NAO)
?cor.test
?cor
cat("Correlation between Montana Temperatures and NAO Index:", cor_Montana_NAO, "\n")
# Chi Plot - Montana Temperatures vs NAO Index
chiplot(data_Montana_NAO, main1 = "Chi Plot - Montana Temperatures vs NAO Index")
# Create a data frame for Zermatt vs Montana Temperatures
data_Zermatt_Montana <- data.frame(Zermatt = Z$Zermatt, Montana = Z$Montana)
# Chi Plot - Zermatt vs Montana Temperatures
chiplot(data_Zermatt_Montana, main1 = "Chi Plot - Zermatt vs Montana Temperatures")
# Chi Plot - Montana Temperatures vs NAO Index
chiplot(data_Montana_NAO, main1 = "Chi Plot - Montana Temperatures vs NAO Index")
source(here::here("script/setup.R"))
##### Download daily NAO measurements
NAO.daily <-
fread('ftp://ftp.cdc.noaa.gov/Public/gbates/teleconn/nao.reanalysis.t10trunc.1948-present.txt')
NAO.daily <- as.matrix(NAO.daily)
colnames(NAO.daily) <- c("year","month","day","NAO")
##### Download temperature data: be sure to work in the correct directory
months <- c(12,1,2) #keep only winter observations
temp_max_Zermatt           <- read_delim("daily_maximum_Zermatt/order_107669_data.txt",
";", escape_double = FALSE, col_types =
cols(time = col_number()),
trim_ws = TRUE, skip = 1)
##### Download daily NAO measurements
NAO.daily <-
fread('ftp://ftp.cdc.noaa.gov/Public/gbates/teleconn/nao.reanalysis.t10trunc.1948-present.txt')
NAO.daily <- as.matrix(NAO.daily)
colnames(NAO.daily) <- c("year","month","day","NAO")
##### Download temperature data: be sure to work in the correct directory
months <- c(12,1,2) #keep only winter observations
temp_max_Zermatt           <- read_delim("data/daily_maximum_Zermatt/order_107669_data.txt",
";", escape_double = FALSE, col_types =
cols(time = col_number()),
trim_ws = TRUE, skip = 1)
##### Download daily NAO measurements
NAO.daily <-
fread('ftp://ftp.cdc.noaa.gov/Public/gbates/teleconn/nao.reanalysis.t10trunc.1948-present.txt')
NAO.daily <- as.matrix(NAO.daily)
colnames(NAO.daily) <- c("year","month","day","NAO")
##### Download temperature data: be sure to work in the correct directory
months <- c(12,1,2) #keep only winter observations
temp_max_Zermatt           <- read_delim("data/practical_3/daily_maximum_Zermatt/order_107669_data.txt",
";", escape_double = FALSE, col_types =
cols(time = col_number()),
trim_ws = TRUE, skip = 1)
setwd("~/Desktop/GitHub/Risk-Analytics")
##### Download daily NAO measurements
NAO.daily <-
fread('ftp://ftp.cdc.noaa.gov/Public/gbates/teleconn/nao.reanalysis.t10trunc.1948-present.txt')
NAO.daily <- as.matrix(NAO.daily)
colnames(NAO.daily) <- c("year","month","day","NAO")
##### Download temperature data: be sure to work in the correct directory
months <- c(12,1,2) #keep only winter observations
temp_max_Zermatt           <- read_delim("data/practical_3/daily_maximum_Zermatt/order_107669_data.txt",
";", escape_double = FALSE, col_types =
cols(time = col_number()),
trim_ws = TRUE, skip = 1)
wd
setwd
months <- c(12,1,2) #keep only winter observations
temp_max_Zermatt           <- read_delim("data/practical_3/daily_maximum_Zermatt/order_107669_data.txt",
";", escape_double = FALSE, col_types =
cols(time = col_number()),
trim_ws = TRUE, skip = 1)
months <- c(12,1,2) #keep only winter observations
temp_max_Zermatt           <- read_delim(here::here("data/practical_3/daily_maximum_Zermatt/order_107669_data.txt"),
";", escape_double = FALSE, col_types =
cols(time = col_number()),
trim_ws = TRUE, skip = 1)
##### Download daily NAO measurements
NAO.daily <-
fread('ftp://ftp.cdc.noaa.gov/Public/gbates/teleconn/nao.reanalysis.t10trunc.1948-present.txt')
NAO.daily <- as.matrix(NAO.daily)
colnames(NAO.daily) <- c("year","month","day","NAO")
##### Download temperature data: be sure to work in the correct directory
months <- c(12,1,2) #keep only winter observations
temp_max_Zermatt           <- read_delim(here::here("data/practical_3/daily_maximum_Zermatt/order_107669_data.txt"),
";", escape_double = FALSE, col_types =
cols(time = col_number()),
trim_ws = TRUE, skip = 1)
colnames(temp_max_Zermatt) <- c("station","time","temp")
temp_max_Zermatt           <- temp_max_Zermatt[-1,]
temp_max_Zermatt[,2]       <- as.Date(apply(temp_max_Zermatt[,2],1,as.character),
"%Y%m%d")
temp_max_Montana           <- read_delim(here::here("data/practical_3/daily_maximum_Montana/order_107668_data.txt"),
";", escape_double = FALSE, col_types =
cols(time = col_number()),
trim_ws = TRUE, skip = 1)
colnames(temp_max_Montana) <- c("station","time","temp")
temp_max_Montana           <- temp_max_Montana[-1,]
temp_max_Montana[,2]       <- as.Date(apply(temp_max_Montana[,2],1,as.character),
"%Y%m%d")
###match the dates of the two time series
temp_max_Montana           <- temp_max_Montana[match(as.matrix(temp_max_Zermatt[,2]),
as.matrix(temp_max_Montana[,2])),]
temp_max_Montana           <- as.matrix(temp_max_Montana)
colnames(temp_max_Montana) <- c("station","time","temp")
temp_max_Zermatt           <- as.matrix(temp_max_Zermatt)
colnames(temp_max_Zermatt) <- c("station","time","temp")
###keep only winter dates
temp_max_Montana <- temp_max_Montana[which(month(as.POSIXlt(temp_max_Montana[,"time"],
format="%Y-%m-%d")) %in% months),]
temp_max_Zermatt <- temp_max_Zermatt[which(month(as.POSIXlt(temp_max_Zermatt[,"time"],
format="%Y-%m-%d")) %in% months),]
Date       <- function( length = 0 ){
newDate = numeric( length )
class(newDate) = "Date"
return(newDate)
}
season_day                   <- yday(as.Date(temp_max_Montana[,2]))
season_day[season_day < 61]  <- season_day[season_day < 61] + 31
season_day[season_day > 334] <- season_day[season_day > 334]- 334
NAO.date <- Date(nrow(NAO.daily))
for(i in 1:nrow(NAO.daily)){
NAO.date[i] <- as.Date(paste(as.character(NAO.daily[i,1]),"-",as.character(NAO.daily[i,2]),
"-",as.character(NAO.daily[i,3]),sep=""),format="%Y-%m-%d")
}
NAO.daily <- mutate(as.tibble(NAO.daily), date = make_date(year, month, day))
index <- as.Date(intersect(as.Date(temp_max_Montana[,2]),as.Date(NAO.date)))
nao <- NAO.daily[which(NAO.daily$date %in% index), 4]
nao <- as.vector(nao)
#Montana
x_Montana          <- data.frame("time"=temp_max_Montana[,2],
"nao"=nao,
"d"=season_day,
"temp"=temp_max_Montana[,3])
as.numeric.factor  <- function(x) {as.numeric(levels(x))[x]}
x_Montana[,"temp"] <- as.numeric(x_Montana[,"temp"])
x_Montana[,"time"] <- as.numeric(year(as.POSIXlt(x_Montana[,"time"], format="%Y-%m-%d")))
x_Montana[,"time"] <- (x_Montana[,"time"]-min(x_Montana[,"time"]))/(max(x_Montana[,"time"])-
min(x_Montana[,"time"]))
#Zermatt
x_Zermatt          <- data.frame("time"=temp_max_Zermatt[,2],
"nao"=nao,
"d"=season_day,
"temp"=temp_max_Zermatt[,3])
as.numeric.factor  <- function(x) {as.numeric(levels(x))[x]}
x_Zermatt[,"temp"] <- as.numeric(x_Zermatt[,"temp"])
x_Zermatt[,"time"] <- as.numeric(year(as.POSIXlt(x_Zermatt[,"time"], format="%Y-%m-%d")))
x_Zermatt[,"time"] <- (x_Zermatt[,"time"]-min(x_Zermatt[,"time"]))/(max(x_Zermatt[,"time"])-
min(x_Zermatt[,"time"]))
Z <- data.frame("Montana"=x_Montana[,"temp"] , "Zermatt"=x_Zermatt[,"temp"], "NAO"=x_Zermatt[,"NAO"])
# Scatterplot of Zermatt values against Montana values
plot(Z$Montana, Z$Zermatt, xlab = "Montana Temperature", ylab = "Zermatt Temperature",
main = "Zermatt vs Montana Temperatures")
abline(lm(Z$Zermatt ~ Z$Montana), col = "red")
legend("topright",
legend = "Fitted Regression Line",
col = "red",
lty = 1,
cex = 0.8)
# Scatterplot of Montana values against NAO values
plot(Z$Montana, Z$NAO, xlab = "Montana Temperature", ylab = "NAO Index",
main = "Montana Temperature vs NAO Index")
abline(lm(Z$NAO ~ Z$Montana), col = "red") # Add a linear regression line
legend("topright",
legend = "Fitted Regression",
col = "red",
lty = 1,
cex = 0.8)
cor_Zermatt_Montana <- cor(Z$Zermatt, Z$Montana)
cat("Correlation between Zermatt and Montana Temperatures:", cor_Zermatt_Montana, "\n")
cor.test(Z$Zermatt, Z$Montana)
# Correlation between Montana Temperatures and NAO Index
cor_Montana_NAO <- cor(Z$Montana, Z$NAO)
cat("Correlation between Montana Temperatures and NAO Index:", cor_Montana_NAO, "\n")
# correlation test
cor.test(Z$Montana, Z$NAO) # significant
cor.test(Z$Montana, Z$NAO) # significant
cor.test(Z$Montana, Z$NAO) # significant
cor.test(Z$Zermatt, Z$Montana)
##### Download daily NAO measurements
NAO.daily <-
fread('ftp://ftp.cdc.noaa.gov/Public/gbates/teleconn/nao.reanalysis.t10trunc.1948-present.txt')
NAO.daily <- fread('ftp://ftp.cdc.noaa.gov/Public/gbates/teleconn/nao.reanalysis.t10trunc.1948-present.txt')
source(here::here("script/setup.R"))
##### Download daily NAO measurements
NAO.daily <- fread('ftp://ftp.cdc.noaa.gov/Public/gbates/teleconn/nao.reanalysis.t10trunc.1948-present.txt')
source(here::here("script/setup.R"))
##### Download daily NAO measurements
NAO.daily <- fread('ftp://ftp.cdc.noaa.gov/Public/gbates/teleconn/nao.reanalysis.t10trunc.1948-present.txt')
NAO.daily <- fread('ftp://ftp.cdc.noaa.gov/Public/gbates/teleconn/nao.reanalysis.t10trunc.1948-present.txt')
NAO.daily <- fread('ftp://ftp.cdc.noaa.gov/Public/gbates/teleconn/nao.reanalysis.t10trunc.1948-present.txt')
=======
plot(sort(pseudo_samples_e), main = "Engie Pseudo-samples", xlab = "Index", ylab = "Log Returns", col = "red", pch = 16)
plot(sort(pseudo_samples_v), main = "Veolia Pseudo-samples", xlab = "Index", ylab = "Log Returns", col = "red", pch = 16)
# Scatter for both pseudo samples
scatter.smooth(x = pseudo_samples_e, y = pseudo_samples_v, xlab = "Engie pseudo", ylab = "Veolia pseudo")
cdf_engie <- ecdf(log_returns_engie)
cdf_veolia <- ecdf(log_returns_veolia)
plot(cdf_engie, col = "red", main = "Empirical Cumulative Distributions")
lines(cdf_veolia, col ="blue")
plot(cdf_engie, col = "red", main = "Empirical Cumulative Distributions")
plot(cdf_engie, col = "red", main = "Empirical Cumulative Distributions")
lines(cdf_veolia, col ="blue")
legend("bottomright", legend = c("Engie", "Veolia"), col = c("red", "blue"), lty = 1)
# Probabilities for cdf_engie and cdf_veoila
u1 <- pobs(log_returns_engie)
u2 <- pobs(log_returns_veolia)
data <- data.frame(engie_prob = u1, veolia_prob = u2)
data_matrix <- as.matrix(data)
# Fit Gaussian copula
gaussian_copula <- normalCopula(dim = 2)
fit_gaussian <- fitCopula(gaussian_copula, data, method = "mpl")
# Fit t copula
t_copula <- tCopula(dim = 2, df = 3)
fit_t <- fitCopula(t_copula, data, method = "mpl")
# Fit Gumbel copula
gumbel_copula <- gumbelCopula(dim = 2)
fit_gumbel <- fitCopula(gumbel_copula, data, method = "mpl")
# Fit Clayton copula
clayton_copula <- claytonCopula(dim = 2, param = 2)
fit_clayton <- fitCopula(clayton_copula, data, method = "mpl")
# Calculate AIC values
aic_values <- AIC(fit_gaussian, fit_t, fit_gumbel, fit_clayton)
aic_values
min(aic_values$AIC) ###the t distribution fits the best our data
install.packages("psych")
library(psych)
df1 <- df(log_returns_engie, log_returns_veolia)
df1 <- df( ,c(log_returns_engie, log_returns_veolia))
df1 <- as.data.frame( ,c(log_returns_engie, log_returns_veolia))
df1 <- as.data.frame(log_returns_engie, log_returns_veolia)
log_returns_engie
df1 <- as.data.frame(log_returns_engie, log_returns_veolia)
df1 <- as.data.frame(log_returns_engie, log_returns_veolia, row.names = NULL)
data.frame(log_returns_engie, log_returns_veolia)
df1 <- data.frame(log_returns_engie, log_returns_veolia)
pairs.panels(df1, method = "kendall")
pseudo_samples <- pobs(df1)
pseudo_samples
# Drawing a scatter plot
par(pty="s")
scatter.smooth(x = log_returns_engie, y = log_returns_veolia, xlab = "Engie negative log returns", ylab = "Veolia negative log returns")
df1 <- data.frame(log_returns_engie, log_returns_veolia)
pairs.panels(df1, method = "kendall")
pseudo_samples <- pobs(df1)
pairs.panels(data, method = "kendall")
pairs.panels(pseudo_samples, method = "kendall")
# Drawing a scatter plot
par(pty="s")
scatter.smooth(x = log_returns_engie, y = log_returns_veolia, xlab = "Engie negative log returns", ylab = "Veolia negative log returns")
# Probabilities for cdf_engie and cdf_veoila
u1 <- pobs(log_returns_engie)
u2 <- pobs(log_returns_veolia)
data <- data.frame(engie_prob = u1, veolia_prob = u2)
data_matrix <- as.matrix(data)
pairs.panels(data, method = "kendall")
# Fit Gaussian copula
gaussian_copula <- normalCopula(dim = 2)
fit_gaussian <- fitCopula(gaussian_copula, data, method = "mpl")
# Fit t copula
t_copula <- tCopula(dim = 2, df = 3)
fit_t <- fitCopula(t_copula, data, method = "mpl")
# Fit Gumbel copula
gumbel_copula <- gumbelCopula(dim = 2)
fit_gumbel <- fitCopula(gumbel_copula, data, method = "mpl")
# Fit Clayton copula
clayton_copula <- claytonCopula(dim = 2, param = 2)
fit_clayton <- fitCopula(clayton_copula, data, method = "mpl")
# Calculate AIC values
aic_values <- AIC(fit_gaussian, fit_t, fit_gumbel, fit_clayton)
aic_values
min(aic_values$AIC) ###the t distribution fits the best our data
BiCopPar2TailDep(family = 2, par = fit_t@copula@parameters[1], par2 = fit_t@copula@parameters[2])
?BiCopPar2TailDep
lambda(fit_t@copula)
BiCopPar2TailDep(family = 2, par = fit_t@copula@parameters[1], par2 = fit_t@copula@parameters[2])
lambda(fit_t@copula)
chiplot(data)
library(evd)
chiplot(data)
?chiplot
chibar(data)
chiplot(data)
fitLambda(data, lower.tail = F)
df_log_returns <- data.frame(log_returns_engie, log_returns_veolia)
chiplot(df_log_returns)
# We compute and show the CDF for both negative log returns
cdf_engie <- ecdf(log_returns_engie)
cdf_veolia <- ecdf(log_returns_veolia)
plot(cdf_engie, col = "red", main = "Empirical Cumulative Distributions")
lines(cdf_veolia, col ="blue")
legend("bottomright", legend = c("Engie", "Veolia"), col = c("red", "blue"), lty = 1)
fitLambda(data, lower.tail = T)
fitLambda(data, lower.tail = F)
fitLambda(data, method = "t", lower.tail = F)
data
BiCopSelect(data[,1], data[,2], familyset=NA)
fitLambda(data_matrix, method = "t", lower.tail = F)
fitLambda(data_matrix, lower.tail = F)
fit_t
lambda(fit_t@copula)
lambda(fit_t@copula)
fitLambda(data_matrix, lower.tail = F)
#Compute the tail dependence parameters through BiCopPar2TailDep and lambda functions
BiCopPar2TailDep(family = 2, par = fit_t@copula@parameters[1], par2 = fit_t@copula@parameters[2])
fitLambda(data_matrix, method = "t", lower.tail = F)
lambda(fit_t@copula)
fitLambda(data_matrix, method = "t", lower.tail = F)
plot(df_log_returns)
chiplot(df_log_returns)
# Create log return function
calculate_log_returns <- function(stocks) {
log_returns <- -diff(log(stocks))
return(log_returns)
}
# Calculate negative log returns for each index
log_returns_engie <- calculate_log_returns(engie$Adj.Close)
plot(log_returns_engie)
log_returns_veolia <- calculate_log_returns(veolia$Adj.Close)
plot(log_returns_veolia)
plot_engie <- plot_ly(x = engie$Date, y = engie$Adj.Close, type = "scatter", mode = "point", name = "Engie") %>%
layout(title = "Engie")
plot_veolia <- plot_ly(x = veolia$Date, y = veolia$Adj.Close, type = "scatter", mode = "point", name = "Veolia") %>%
layout(title = "Veolia")
combined_plot_2 <- subplot(plot_engie, plot_veolia)
layout(combined_plot_2, title = "Comparison of stock indices")
df_log_returns <- data.frame(log_returns_engie, log_returns_veolia)
lambda(fit_t@copula)
fitLambda(data_matrix, method = "t", lower.tail = F)
chiplot(df_log_returns)
#Compute the tail dependence parameters through BiCopPar2TailDep and lambda functions
BiCopPar2TailDep(family = 2, par = fit_t@copula@parameters[1], par2 = fit_t@copula@parameters[2])
# Probabilities for cdf_engie and cdf_veoila
u1 <- pobs(log_returns_engie)
u2 <- pobs(log_returns_veolia)
data <- data.frame(engie_prob = u1, veolia_prob = u2)
data_matrix <- as.matrix(data)
pairs.panels(data, method = "kendall")
# Fit Gaussian copula
gaussian_copula <- normalCopula(dim = 2)
fit_gaussian <- fitCopula(gaussian_copula, data, method = "mpl")
# Fit t copula
t_copula <- tCopula(dim = 2, df = 3)
fit_t <- fitCopula(t_copula, data, method = "mpl")
# Fit Gumbel copula
gumbel_copula <- gumbelCopula(dim = 2)
fit_gumbel <- fitCopula(gumbel_copula, data, method = "mpl")
# Fit Clayton copula
clayton_copula <- claytonCopula(dim = 2, param = 2)
fit_clayton <- fitCopula(clayton_copula, data, method = "mpl")
# Calculate AIC values
aic_values <- AIC(fit_gaussian, fit_t, fit_gumbel, fit_clayton)
aic_values
min(aic_values$AIC) ###the t distribution fits the best our data
BiCopSelect(data[,1], data[,2], familyset=NA)
#Compute the tail dependence parameters through BiCopPar2TailDep and lambda functions
BiCopPar2TailDep(family = 2, par = fit_t@copula@parameters[1], par2 = fit_t@copula@parameters[2])
fitLambda(data_matrix,, lower.tail = F)
plot(log_returns_engie, log_returns_veolia)
lambda(fit_t@copula)
fitLambda(data_matrix, method = "t", lower.tail = F)
chiplot(df_log_returns)
# We compute and show the CDF for both negative log returns
cdf_engie <- pobs(log_returns_engie)
cdf_veolia <- pobs(log_returns_veolia)
plot(cdf_engie, col = "red", main = "Empirical Cumulative Distributions")
lines(cdf_veolia, col ="blue")
legend("bottomright", legend = c("Engie", "Veolia"), col = c("red", "blue"), lty = 1)
# We compute and show the CDF for both negative log returns
cdf_engie <- ecdf(log_returns_engie)
cdf_veolia <- ecdf(log_returns_veolia)
plot(cdf_engie, col = "red", main = "Empirical Cumulative Distributions")
lines(cdf_veolia, col ="blue")
legend("bottomright", legend = c("Engie", "Veolia"), col = c("red", "blue"), lty = 1)
# We compute and show the CDF for both negative log returns
cdf_engie <- pobs(log_returns_engie)
cdf_veolia <- ecdf(log_returns_veolia)
plot(cdf_engie, col = "red", main = "Empirical Cumulative Distributions")
lines(cdf_veolia, col ="blue")
legend("bottomright", legend = c("Engie", "Veolia"), col = c("red", "blue"), lty = 1)
cdf_engie
cdf_veolia
# We compute and show the CDF for both negative log returns
cdf_engie <- ecdf(log_returns_engie)
cdf_engie
plot(cdf_engie, col = "red", main = "Empirical Cumulative Distributions")
# We compute and show the CDF for both negative log returns
cdf_engie <- ecdf(log_returns_engie)
cdf_veolia <- ecdf(log_returns_veolia)
plot(cdf_engie, col = "red", main = "Empirical Cumulative Distributions")
lines(cdf_veolia, col ="blue")
legend("bottomright", legend = c("Engie", "Veolia"), col = c("red", "blue"), lty = 1)
# We compute and show the CDF for both negative log returns
cdf_engie <- ecdf(log_returns_engie)
cdf_engie
# Create log return function
calculate_log_returns <- function(stocks) {
log_returns <- -diff(log(stocks))
return(log_returns)
}
# Calculate negative log returns for each index
log_returns_engie <- calculate_log_returns(engie$Adj.Close)
plot(log_returns_engie)
log_returns_veolia <- calculate_log_returns(veolia$Adj.Close)
plot(log_returns_veolia)
plot_engie <- plot_ly(x = engie$Date, y = engie$Adj.Close, type = "scatter", mode = "point", name = "Engie") %>%
layout(title = "Engie")
plot_veolia <- plot_ly(x = veolia$Date, y = veolia$Adj.Close, type = "scatter", mode = "point", name = "Veolia") %>%
layout(title = "Veolia")
combined_plot_2 <- subplot(plot_engie, plot_veolia)
layout(combined_plot_2, title = "Comparison of stock indices")
df_log_returns <- data.frame(log_returns_engie, log_returns_veolia)
pairs.panels(data, method = "kendall")
library(copula)
library(MASS)
library(tseries)
library(dplyr)
library(plotly)
library(VineCopula)
engie <- read.csv("engie.csv")
veolia <- read.csv("veolia.csv")
# Data cleaning and plot
engie <- engie[-c(4456, 4489), ]
veolia <- veolia[-c(4456, 4489), ]
engie$Adj.Close <- as.numeric(engie$Adj.Close)
veolia$Adj.Close <- as.numeric(veolia$Adj.Close)
par(mfrow = c(1,2))
plot(engie$Adj.Close)
plot(veolia$Adj.Close)
# ADF test to asses the stationarity
adf_engie <- adf.test(engie$Adj.Close) #p-value = 0.4466
adf_veolia <- adf.test(veolia$Adj.Close)#p-value = 0.8228
# Create log return function
calculate_log_returns <- function(stocks) {
log_returns <- -diff(log(stocks))
return(log_returns)
}
# Calculate negative log returns for each index
log_returns_engie <- calculate_log_returns(engie$Adj.Close)
plot(log_returns_engie)
log_returns_veolia <- calculate_log_returns(veolia$Adj.Close)
plot(log_returns_veolia)
plot_engie <- plot_ly(x = engie$Date, y = engie$Adj.Close, type = "scatter", mode = "point", name = "Engie") %>%
layout(title = "Engie")
plot_veolia <- plot_ly(x = veolia$Date, y = veolia$Adj.Close, type = "scatter", mode = "point", name = "Veolia") %>%
layout(title = "Veolia")
combined_plot_2 <- subplot(plot_engie, plot_veolia)
layout(combined_plot_2, title = "Comparison of stock indices")
df_log_returns <- data.frame(log_returns_engie, log_returns_veolia)
# Drawing a scatter plot
par(pty="s")
scatter.smooth(x = log_returns_engie, y = log_returns_veolia, xlab = "Engie negative log returns", ylab = "Veolia negative log returns")
# We compute and show the CDF for both negative log returns
cdf_engie <- ecdf(log_returns_engie)
cdf_veolia <- ecdf(log_returns_veolia)
plot(cdf_engie, col = "red", main = "Empirical Cumulative Distributions")
lines(cdf_veolia, col ="blue")
legend("bottomright", legend = c("Engie", "Veolia"), col = c("red", "blue"), lty = 1)
# Probabilities for cdf_engie and cdf_veoila
u1 <- pobs(log_returns_engie)
u2 <- pobs(log_returns_veolia)
data <- data.frame(engie_prob = u1, veolia_prob = u2)
data_matrix <- as.matrix(data)
pairs.panels(data, method = "kendall")
# Fit Gaussian copula
gaussian_copula <- normalCopula(dim = 2)
fit_gaussian <- fitCopula(gaussian_copula, data, method = "mpl")
# Fit t copula
t_copula <- tCopula(dim = 2, df = 3)
fit_t <- fitCopula(t_copula, data, method = "mpl")
# Fit Gumbel copula
gumbel_copula <- gumbelCopula(dim = 2)
fit_gumbel <- fitCopula(gumbel_copula, data, method = "mpl")
# Fit Clayton copula
clayton_copula <- claytonCopula(dim = 2, param = 2)
fit_clayton <- fitCopula(clayton_copula, data, method = "mpl")
# Calculate AIC values
aic_values <- AIC(fit_gaussian, fit_t, fit_gumbel, fit_clayton)
aic_values
min(aic_values$AIC) ###the t distribution fits the best our data
BiCopSelect(data[,1], data[,2], familyset=NA)
#Compute the tail dependence parameters through BiCopPar2TailDep and lambda functions
BiCopPar2TailDep(family = 2, par = fit_t@copula@parameters[1], par2 = fit_t@copula@parameters[2])
lambda(fit_t@copula)
fitLambda(data_matrix, method = "t", lower.tail = F)
chiplot(df_log_returns)
>>>>>>> Stashed changes
>>>>>>> bd978b16ee058f5419bfe0324d45a5811e230fbf
