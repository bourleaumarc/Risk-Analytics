---
title: "Practical2"
author: "Marc Bourleau"
date: "2023-10-21"
output: html_document
---

```{r, echo = FALSE, message = FALSE, warning=FALSE}
source(here::here("script/setup.R"))
```

# Part 1: Venice

## (a) Read in the data. Extract and represent the yearly max values from 1940 to 2009. What do you observe ?

```{r}
# Read the Data
venice <- venice90
# Using block maxima approach to have the maximum sea level per year 
venice_max <- venice %>%
  group_by(year) %>%
  summarise(max_sea_level = max(sealevel))
head(venice_max)
plot_ly(venice_max, x = ~year, y = ~max_sea_level, type = 'scatter', mode = 'markers', name = 'Max Value') %>% layout(title = "Maximum Value per year", xaxis = list(title="Year"), yaxis = list(title="Maximum Value"))  %>% add_segments(x = 1940, xend = 2009, y = 140, yend = 140, line = list(color = 'red', width = 2))

```

From the plot, we can discern that there are a total of 11 data points that surpass the 140 cm threshold, marked by the red line. The highest recorded sea level, occurring in 1966, is notably distinct as the peak value in the dataset, reaching 192 cm. When considering the distribution of the maximum values each year, there's a hint of a potential trend in the data. Whether extreme values are stationary or not will be investigated in (D) using likelihood ratio test.

## (b) We are end of 2009 and would like to predict the yearly maximum values over the next 13 years (from 2010 to 2022). A naive approach consists of fitting a linear model on the observed yearly maxima and predict their values for 2010-2022. Proceed to this prediction and provide confidence intervals.

```{r}
# Create linear model 
mod1 <- lm(max_sea_level ~ year, data = venice_max)
summary(mod1)

# Predictions of 13 next years using the linear model 
mod1_predict <- predict.lm(mod1,newdata=data.frame("year"=c(2010:2022)),se=T, interval = "confidence", level = 0.95)
mod1_predict

```

As anticipated, the simple approach leads to poor predictions, with confidence intervals spanning from 124 to 146. The main reason for this lackluster performance is the violation of key assumptions in linear modeling when dealing with extreme values, such as: linearity, homoescedasticity and increased sensitivity to outliers. Consequently, we will turn to specialized models rooted in the realm of extreme value theory to address these challenges more effectively.

## (c) Represent in the same graph the predicted yearly max values for the period 2010-2022, their pointwise confidence bounds and the observed values greater than 140 cm from the table below.

```{r, warning = FALSE}

# Stored the predictions in a dataframe 
venice_max_predict <- data.frame(
  PredictedValues = mod1_predict) %>% 
  mutate(year = c(2010:2022))
head(venice_max_predict)

#Plot the confidence intervals
plotCI(x = venice_max_predict$year,
       y = venice_max_predict$PredictedValues.fit.fit,
       li = venice_max_predict$PredictedValues.fit.lwr,
       ui = venice_max_predict$PredictedValues.fit.upr)

#Create a new dataframe for the extreme values of 2010 - 2022 (table from Wikipedia)
max_real <- data.frame(year = c(2012, 2012, 2013, 2018, 2019, 2019, 2019, 2022), max_sea_level = c(143, 149, 143, 156, 187, 144, 154, 204))

# Create the ggplot object
venice_plot <- ggplot() +
  geom_point(data = max_real, aes(x = year, y = max_sea_level, color = "Observed"), alpha = 0.5, show.legend = TRUE, name = "Observed") +
  geom_point(data = venice_max_predict, aes(x = year, y = PredictedValues.fit.fit, color = "Predicted"), shape = 1, show.legend = TRUE, name = "Predicted") +
  labs(title = "Predicted Yearly Max Values vs Observed Values (>140cm)", x = "Year", y = "Sea Level") +
  scale_x_continuous(breaks = unique(c(venice_max_predict$year, max_real$year))) +
  scale_color_manual(name = "Data Type", values = c("Observed" = "red", "Predicted" = "black")) +
  theme(legend.title = element_blank())

# Convert the ggplot object to a Plotly interactive plot
interactive_venice_plot <- ggplotly(venice_plot)

# Add the confidence interval as a separate trace
interactive_venice_plot <- interactive_venice_plot %>%
  add_ribbons(data = venice_max_predict, x = ~year, ymin = ~PredictedValues.fit.lwr, ymax = ~PredictedValues.fit.upr, color = I("blue"), showlegend = TRUE, name = "Confidence Interval")

# Display the interactive plot
interactive_venice_plot

```

As previously noted and showcased in the plot, the predictions of extreme values using a linear model exhibit significant shortcomings. The predicted values follow a linear pattern and consistently fall short of accurately capturing extreme events. There is one exception, the observation on the 13th of November 2019, for which the Confidence Interval of the predicted value aligns well with the actual value. Nonetheless, even in this case, the model underestimates the observed value by approximately 9 cm, a substantial deviation.

To address these limitations, we are turning to the Generalized Extreme Value (GEV) distribution, a more suitable approach for modeling extreme events.

## (d) Fit a GEV a with constant parameters to the historical yearly max values. Fit a GEV with time varying location parameter. Compare the two embedded models using likelihood ratio test (LRT). Show diagnostic plots.

```{r}
########################################## extRemes ##########################################
# First we unlist our data frame
list_max_sea_levels <- unlist(venice_max$max_sea_level)
# GEV model with fixed location 
gev_fix <- fevd(list_max_sea_levels, type = "GEV", time.units= "year")
plot(gev_fix) ; gev_fix$results$par ; return.level(gev_fix) # shape is almost equal to 0.
# Compute confidence interval 
ci(gev_fix, type= "parameter") # CI includes 0
# GEV model with varying time-location using both linear and harmonic function 
yy <- 1:length(venice_max$year) 
gev_time_varying_linear <- fevd(venice_max$max_sea_level, venice_max, type = "GEV", time.units= "year", location.fun = ~ yy)
gev_time_varying_harmonic <- fevd(venice_max$max_sea_level, venice_max, type = "GEV", time.units= "year", location.fun = ~sin(2 * pi * (year - 1940)/70) + cos(2 * pi * (year - 1940)/70))

plot(gev_time_varying_linear) ; plot(gev_time_varying_harmonic) ; gev_time_varying_linear$results$par ; gev_time_varying_harmonic$results$par;  return.level(gev_time_varying_linear) ; return.level(gev_time_varying_harmonic) 
# Compare the two models using likelihood ratio test: Ho: no significant difference in model fit H1: there is a significant difference in model fit between the two models.
lrt_result_linear <- lr.test(gev_fix, gev_time_varying_linear) # we can almost reject the null hypothesis at 95% significant level, which make sense as we have indications that the distribution is non-stationary
lrt_result_harmonic <-lr.test(gev_fix, gev_time_varying_harmonic) 
attributes(gev_time_varying_linear)
# 589.5372 linear --> this is sufficient as lower AIC 
# 594.9147 harmonic --> too complex, we stick with the linear (time varying location)
```

We initially observed that the shape parameter of the fixed Generalized Extreme Value (GEV) model was close to 0. This suggested that the extreme values closely follow a Gumbel distribution, which is characterized by an exponential tail. To confirm this hypothesis, we calculated a confidence interval using the ci function in the extRemes package. The resulting confidence interval included 0, indicating that the shape parameter was not significantly different from 0.

Next, we aimed to fit a GEV model with time-varying location parameters. We explored two different functions: linear and harmonic. The linear function assumed that "Year" is treated as a linear predictor, and the location parameter is assumed to change linearly over time. However, the likelihood ratio test indicated that the fixed GEV model (stationary) was a better fit. This result made sense because while there were some patterns in the maximum values, the linear function did not capture the entire variability of the data.

On the other hand, the harmonic function, which is often used to model periodic phenomena, yielded a different outcome. It allowed us to reject the null hypothesis of the likelihood ratio test, indicating that the GEV model with time-varying location parameters was a better fit. This result suggested that our maximum values displayed non-stationary behavior.

## (e) Add if necessary a time varying scale and or shape GEV parameter. Select the best model according to LRT.

```{r}
# Fit a GEV model with time-varying scale 
gev_time_varying_scale <-  fevd(venice_max$max_sea_level, venice_max, type = "GEV", time.units = "year", scale.fun = ~ yy) # AIC: 600
gev_time_varying_shape <- fevd(venice_max$max_sea_level, venice_max, type = "GEV", time.units = "year", shape.fun = ~ yy) # AIC:  600.7072 
gev_time_varying_scale_shape <- fevd(venice_max$max_sea_level, venice_max, type = "GEV", time.units = "year", shape.fun = ~ yy, scale.fun = ~ yy ) # AIC: 601.9
gev_time_varying_location_scale <- fevd(venice_max$max_sea_level, venice_max, type = "GEV", time.units = "year", scale.fun = ~ yy, location.fun =  ~ yy) # AIC: 590.6
gev_time_varying_location_shape <- fevd(venice_max$max_sea_level, venice_max, type = "GEV", time.units = "year", shape.fun = ~ yy, location.fun =  ~ yy) # AIC: 586
gev_time_varying_scale_shape_location <- fevd(venice_max$max_sea_level, venice_max, type = "GEV", time.units = "year", shape.fun = ~ yy, location.fun =  ~ yy, scale.fun = ~ yy ) # AIC: 586 
lrt_result <- lr.test(gev_time_varying_location_shape, gev_time_varying_scale_shape_location) # can't reject the null hypothesis so we decide to choose the gev_time_varying_location_shape 

```

After comparing the AIC of the models above, it aprears that the best models are the "gev_time_varying_location_shape" and "gev_time_varying_scale_shape_location", for which we did the LRT test. it shows us that the best model is the less complex one: the "gev_time_varying_location_shape".

## (f) Predict the 13-years return level, each year from 2010 to 2022.

```{r}
return_levels_df <- data.frame(return_level = numeric())  # Create an empty data frame with a column "return_level"

for (i in 71:83) {
  loc <- gev_time_varying_location_shape$results$par[1] + gev_time_varying_location_shape$results$par[2] * i
  scale <- gev_time_varying_location_shape$results$par[3]
  shape <- gev_time_varying_location_shape$results$par[4] + gev_time_varying_location_shape$results$par[5] * i

  return_level <- qgev(1 - 1/13, location = loc, scale = scale, shape = shape)
  return_levels_df <- rbind(return_levels_df, data.frame(return_level = return_level))
}

# Assign sequential numeric row names
rownames(return_levels_df) <- 1:nrow(return_levels_df)

print(return_levels_df)

 

```

```{r}
rlci<- data.frame(return_level = numeric(), Lower_CI = numeric(), Upper_CI = numeric())
B <- 200
rt1 <- matrix(0, nrow=B, ncol=13)


for(j in 1:B){
  for (i in 1:13){

    data <- suppressWarnings(rgev(70, loc = gev_time_varying_location_shape$results$par[1] + gev_time_varying_location_shape$results$par[2] * (70+i), scale = gev_time_varying_location_shape$results$par[3], shape = gev_time_varying_location_shape$results$par[4] + gev_time_varying_location_shape$results$par[5] * (70+i)))
    
  fit <- gev.fit(data)
  
  rt1[j,i] <- qgev(1-1/13, location = fit$mle[1], scale = fit$mle[2], shape = fit$mle[3])
  
  }}


rt <- apply(rt1,2,function(x){quantile(x,0,5)})
rtl <- apply(rt1,2,function(x){quantile(x,0,025)})
rtu <- apply(rt1,2,function(x){quantile(x,0,975)}) 
  
rlci <- rbind(rlci, data.frame(return_level = rt, Lower_CI = rtl, Upper_CI = rtu))


plot(1:13, rt)
lines(1:13, rtl, lty = 2, col = 2)
lines(1:13, rtu, lty = 1)

```

## (h) Represent in the same graph your predictions of the 13-years return levels, their pointwise confidence intervals, the predicted yearly max values from the linear model and the observed values greater than 140 cm from the table below.

```{r}
rlci$Year = c(2010:2022)
return_levels_df$Year = c(2010:2022)

interactive_venice_plot %>% 
  add_lines(data = rlci, x = ~Year, y = ~return_level, color = I("red"), name = "Predicted values (model)") %>%
  add_ribbons(data = rlci, x = ~Year, ymin = ~Lower_CI, ymax = ~Upper_CI, color = I("red"), name  = "Confidence Interval (model)")
```

Here, we can observe a trend slowly increasing from the predicted values. However, there are a few problems with the method we used. Indeed, the confidence intervals are way too close to the return level, giving us bounds which are way too narrow to observe anything significant.

## (i) Broadly speaking, each year, there is a chance of 1/13 that the observed value is above the 13- years return level. Comment the results for both the linear model prediction and GEV approach. Note that 12 of the 20 events occurred in the 21st century.

The main issue we see is that the observed values from 2010 to 2022 are much higher than the expected return level. This indicates that the models we are using are crucially underestimating how fast the sea level is currently rising, most notably the extreme values.

The linear model predictions did not give us something reliable enough, as it did not take into account its parameters varying. As a result, only one of the observed values is inside of the confidence intervals.

the return levels from the GEV model are giving us better results overall, but still far from the reality. If we imagine similarly wide confidence intervals for the GEV (ignoring the limitations we had) compared to the linear model, we could potentially have up to 6/8 values inside its bounds, which would be somewhat good, except for the two highest values (12/11/2019 and 22/11/2022), which are on a scope that wasn't even imagined by the model.

In conclusion, the models we used until now were all incomplete, and could not predict the massive change the sea levels underwent, most notably in the latest 3 years where the values attained heights never seen before. This event can be explained through the effects of global warming, accelerating the pace at which the sea level rises worldwide.

# Part 2: Nuclear reactors

## (a) Read in the data. Display a time series plot of the water level across the data range and try to identify times of highest levels.

```{r time series plot}
# Load the data
load(here::here("data/niveau.Rdata"))

# Convert to Date format
niveau$Zeitstempel <- as.Date(niveau$Zeitstempel)
niveau$Zeitpunkt_des_Auftretens <- as.Date(niveau$Zeitpunkt_des_Auftretens)

# Identifying points above the threshold
threshold <- 328
above_threshold <- niveau[niveau$Wert > threshold, ]

# Create a base plot
p <- plot_ly(data = niveau, x = ~Zeitstempel, y = ~Wert, type = 'scatter', mode = 'lines',
             name = 'Water Level', hoverinfo = 'x+y')

# Add points above the threshold
p <- add_trace(p, data = above_threshold, x = ~Zeitstempel, y = ~Wert, mode = 'markers',
               marker = list(color = 'red', size = 10), name = 'Above Threshold')

# Customize layout
p <- layout(p, title = 'Time Series Plot of Water Levels',
            xaxis = list(title = 'Date'),
            yaxis = list(title = 'Water Level (m)'))

# Show the plot
p

```

To pinpoint the periods with the highest water levels, we've established a threshold of 327 meters (95% quantile). Upon analysis, the peak water level was observed in August 2007, surpassing this predefined threshold, with a value of 329.32 meters. Furthermore, in July 2021, there were five instances where the water levels exceeded the established threshold of 327 meters.

## (b) Now display a histogram of the water levels. What do you observe about the distribution?

```{r}
# Plot histogram with custom axis
hist(niveau$Wert, breaks = 30, col = "skyblue", xlab = "Water Level (m)", 
     main = "Histogram of Water Levels", xaxt = 'n')  # Turn off default x-axis

# Adding a line for mean
mean_value <- mean(niveau$Wert)
abline(v = mean_value, col = "red", lwd = 2)

# Adding custom x-axis with decimals
axis(1, at = seq(floor(min(niveau$Wert)), ceiling(max(niveau$Wert)), by = 0.5), las = 2)

# Adding a legend
legend("topright", legend = paste("Mean =", round(mean_value, 2)), 
       col = "red", lwd = 2)


```

The histogram reveals a right-skewed distribution, indicating a higher frequency of lower water level readings and a gradual decrease in frequency as the levels rise. Most of the observed water levels concentrate below 327 meters. Notably, the distribution's tail extends to the right, suggesting infrequent occurrences of substantially elevated water levels. These exceptional values could represent sporadic episodes of extreme water conditions.

## (c) Explain how you would model the high water levels using a peaks-over-threshold approach.

The peaks-over-threshold (POT) approach is a statistical method used to model extreme values in a dataset. It focuses on the tail of the distribution, where the extreme events reside. To model high water levels using the POT approach, I would follow these steps:

1.  **Select a threshold**: choose an appropriate threshold above which water level readings are considered extreme and above which the plot is roughly linear. This threshold should be high enough to exclude mundane variations but not so high that you have too few data points to model accurately.

2.  **Extracting exceedances**: once the threshold is set, identify all the data points that exceed it. These are the "peaks" you will analyze. Use the mean excess plot.

3.  **Fitting a Distribution**: fit a Generalized Pareto Distribution (GPD) to the exceedances. The fitting can be done using maximum likelihood estimation or Bayesian methods.

4.  **Parameter Estimation**: which are the shape parameter (kappa), the scale parameter (sigma), and the location parameter (theta). These parameters define the behavior of the distribution's tail.

5.  **Model Diagnostics**: after fitting the model, perform diagnostic checks to ensure that the GPD provides a good fit to the data: analyzing residual plots, conducting goodness-of-fit tests, and using plots comparing the empirical and theoretical exceedance probabilities.

6.  **Clustering:** apply the Ferro-Segers estimate, using the \*\*\`extremalindex\`\*\* function in R, counts exceedances of a high threshold within time windows. A high extremal index (\\\>1) suggests clustering of exceedances, indicating a need for declustering.

7.  **Estimating Return Levels**: estimate high quantiles (return levels) and their corresponding return periods. The return level is the value that is expected to be exceeded once every specific period (e.g., a 100-year flood level).

8.  **Assessing Uncertainty**: confidence intervals for the return levels can be constructed, use `confint`.

## (d) Comment on the aspect of clustering of extremes. How do you propose to measure and deal with clustering of the daily water levels?

Clustering of extremes refers to the tendency of extreme events to occur in sequences or bursts rather than being uniformly distributed over time. This is particularly relevant in environmental data, such as water levels, where extreme events can be dependent on preceding conditions (ex: Chaux-de-fonds heavy rainfall during 3 days).

To measure and deal with clustering in daily water levels, we propose the following steps:

**Measuring clustering**:

1.  **Run Tests for Serial Dependence**: use statistical tests like the autocorrelation function (ACF), or the Ljung-Box test to detect serial dependence in the data above the threshold.
2.  **Identify Clusters**: use the `extremalindex` function to determine whether. The "declustering" period---how much time must pass without an exceedance before a new cluster is defined---needs to be determined.
3.  **Declustering Methods**: apply declustering algorithms to separate the data into independent clusters of exceedances and intervening non-exceedances. A straightforward declustering method, is to use a for loopto identify and group extreme precipitation events by their occurrence in distinct years,and then use the `decluster` function from `extRemes` package.

**Dealing with clustering:**

1.  **Declustering the Data**: before fitting a model to the exceedances, decluster the data to remove the influence of serial dependence. Each cluster may be represented by its maximum value.
2.  **Modeling with Clustering**
3.  **Intensity Measures**: such as the mean number of exceedances per cluster or the average duration of clusters into the risk assessment.
4.  **Time-Varying Thresholds**: consider dynamic thresholds that adjust for seasonality or other known factors that influence clustering, rather than a fixed threshold.

## (e) Perform the analysis you suggest in c) and d) and compute the 50- and 100-year return levels. Explain your choice of threshold and provide an estimate of uncertainty for the return levels. Note: take care to compute the return level in yearly terms.

```{r, warning = False}
# 1. Select Threshold: using mean residual life plot, quantile and threshrange.plot
plot(niveau$Wert)
meplot(niveau$Wert, at = pretty(niveau$Wert, n = 10), labels = format(pretty(niveau$Wert, n = 10), nsmall = 1))
mrlplot(niveau$Wert,main="Mean Residual Life Plot")
quantile(niveau$Wert, 0.95) # 327
threshrange.plot(niveau$Wert, r = c(325, 329), nint = 16) # Choose 327
threshold_clustering <- 327 # select 327 as the threshold
# 2. Extract Exceedances
exceedances <- niveau$Wert[niveau$Wert > threshold_clustering]
# 3. Fit distribution over a range of thresholds and parameter estimation: GPD
fitMLE <- fevd(as.vector(exceedances), method = "MLE",  type="GP", threshold = threshold_clustering)
# 4. Model Diagnostics
plot(fitMLE)
# 5. Check if there is clustering of extremes (Ferro-Segers estimate)
extremalindex(niveau$Wert, threshold_clustering) # 1 / 0.156 = 6.4; we have a very large value (which is very logical as we are dealing with precipitation data), indicating a need to decluster
# 6. Declustering 
years <- c()
k <- 1
for(i in 1:nrow(niveau)){
  if(is.na(niveau$Wert[i])){
    next
  }else{
    years[k] <- year(niveau$Zeitstempel[i])
    k <- k+1
  }
}
years <- years-1999
decl1 <- decluster(as.vector(niveau$Wert), threshold = threshold_clustering, groups=years, na.action=na.omit)
plot(decl1)
decl1
# Add years and decl1 to niveau data frame 
niveau$years <- years
niveau$decl1 <- decl1
# 7. Estimate Return Levels
# Assuming 1 observation per day, we prefer using fpot from evd
rl_mle_evd_50 <- fpot(as.vector(niveau$Wert), threshold = threshold_clustering, npp = 365.25, mper=50)
rl_mle_evd_100 <- fpot(as.vector(niveau$Wert), threshold = threshold_clustering, npp = 365.25, mper=100)
plot(rl_mle_evd_50) ; plot(rl_mle_evd_100, main = "100-Year Return Level")
rl_mle <- return.level(fitMLE, conf = 0.05, return.period= c(2,5,10,20,50,100), do.ci=T)
# Plotting the 50-year return level
par(mfrow = c(2, 2))
plot(rl_mle_evd_50)
title(main = "50-Year Return Level", line = -1, col.main = "red")
plot(rl_mle_evd_100)
title(main = "100-Year Return Level", line = -1, col.main = "red")
par(mfrow = c(1, 1))
# 8. Assessing uncertainty
```
This threshold was determined by examining the mean residual life plot, where 327 meters emerged as a critical point where the mean excess values begin to show significant variation. This value represents a transition in the data, marking where the mean excess values start to widen noticeably. This observation is further substantiated by the fact that 327 meters corresponds to the 95th percentile of your dataset, indicating it is a level exceeded by only the top 5% of your observations. 

## (f) Explain the drawbacks and advantages of using a block maxima method instead of the one used in c)-e).

# Part 3: Night temperatures in Lausanne

## (a) Read in the data for the daily night maximum temperatures in Lausanne. Subset the summer months (June to September).
```{r}
night_max <- read_csv(here::here("data/nightmax.csv"))  
night_min <- read_csv(here::here("data/nightmin.csv"))
# Remove rows with missing values
night_max <- na.omit(night_max)
night_min <- na.omit(night_min)
# Subset the data for summer months (June to September)
summer_night_max <- night_max[format(night_max$date,"%m") %in% c("06", "07", "08", "09"), ] 
```
## (b) Assess whether extremes of the subsetted series in (a) occur in cluster.
```{r question 3b}
# Visualise the data: plot and histogram of the maximum temperatures for the summer months 
plot(summer_night_max$date, summer_night_max$night.max, type = "b", xlab = "Date", ylab = "Night Max Temperature", main = "Summer Night Max Temperatures")
hist(summer_night_max$night.max, breaks = 30, xlab = "Night Max", ylab = "Frequency", main = "Histogram of summer night max temperatures")
# Have more information about the distribution 
min(summer_night_max$night.max) ; mean(summer_night_max$night.max); max(summer_night_max$night.max)
quantile(summer_night_max$night.max, 0.95) # doesn't seem to be very right-skewed 
# Choose the threshold using the mrlplot()
mrlplot(summer_night_max$night.max, main="Mean residual")
threshrange.plot(summer_night_max$night.max, r= c(20,30), nint = 20) # we are going to use 30 
th_1 <- 30 # with this threshold we would have 5% of observations: 400 so seems good. 
# Visualise the threshold 
plot(summer_night_max$night.max, type = 'l')
abline(h=th_1,col=2) # looks good 
# Assess the threshold
pot_mle <- fevd(summer_night_max$night.max, method = "MLE", type="GP", threshold=th_1) 
plot(pot_mle) 
rl_mle <- return.level(pot_mle, conf = 0.05, return.period= c(2,5,10,20,50,100), do.ci=T) # diagnostic plot looks good and confidence interval become wider the longer the return level, which make sense 
# Return level plots with MLE 
par(mfcol=c(1,1))
plot(pot_mle, type="rl",
     main="Return Level Plot for Oberwang w/ MLE",
    ylim=c(0,200), pch=16)
loc <- as.numeric(return.level(pot_mle, conf = 0.05, return.period=50))
segments(50, 0, 50, loc, col= 'midnightblue',lty=6)
segments(0.01,loc,50, loc, col='midnightblue', lty=6)
# Assess wether extremes occur in a cluster using extremalindex()
extremalindex(night_max$night.max, th_1) # 0.31 so 1/0.31 = 3.23 WE NEED TO DECLUSTER 


```
We have an approximate cluster size of 3.23, indicating that extremes values are clustered together. Therefore, we need to decluster. 

## (c) Decluster the data from (a) using a suitable threshold. Plot the resulting declustered data. (Hint: you may want to use the extRemes package.)
```{r question 3c}
# We need to create a vector of size night_max that stores the year. 
years_part3 <- numeric(nrow(night_max))

k <- 1
for (i in 1:nrow(night_max)) {
  if (is.na(night_max$night.max[i])) {
    next
  } else {
    years[k] <- year(as.Date(night_max$date[i]))
    k <- k + 1
  }
}
years_part3 <- years-1999 # we don't want the year but the index of the years
# Use decluster function of extRemes 
decl1 <- decluster(night_max$night.max, threshold=th_1, groups=years_part3, na.action=na.omit) # vector and groups need to have the same size 
decl1 # we have 71 clusters 
plot(decl1) # shows in grey the points that are not retained in the selection 

```
## (d) Fit a GPD to the data, both raw and declustered. Assess the quality of the fit. 
```{r}
# Use fevd of the normal and declustered 
gpd_raw <- fevd(night_max$night.max, threshold = th_1, type = "GP")
gpd_declustered <- fevd(decl1, threshold = th_1, type = "GP")
plot(gpd_raw)
plot(gpd_declustered)
```

## (e) Repeat the above analysis for the negatives of the daily nightly minimum temperatures for the
winter months (November-February).
```{r}
# Subset the winter months (November to February)
winter_night_min <- night_min[format(night_min$date, "%m") %in% c("11", "12", "01", "02"), ]
# We first do the plot and histogram of the negatives of the daily nightly minimum temperatures for the winter months (November-February).
plot(winter_night_min$date, winter_night_min$night.min, type = "l", xlab = "Date", ylab = "Night Min Temperature", main = "Winter Night Min Temperatures")
hist(winter_night_min$night.min, breaks = 30, xlab = "Date", ylab = "Night Min Temperature", main = "Winter Night Min Temperatures frequency")
min(winter_night_min$night.min); mean(winter_night_min$night.min); max(winter_night_min$night.min)
quantile(winter_night_min$night.min, 0,95)

mrlplot(winter_night_min$night.min, main="Mean residual")
threshrange.plot(winter_night_min$night.min, r= c(-5,15), nint =20)

mean(winter_night_min$night.min)

max(summer_night_max$night.max)
quantile(summer_night_max$night.max, 0,95)
mean(summer_night_max$night.max)

```


